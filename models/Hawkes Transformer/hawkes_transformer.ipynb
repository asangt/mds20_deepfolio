{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hawkes_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8WiyCSxdx1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as utils_data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxnUz3QIRzrl"
      },
      "source": [
        "# ALL GLOBAL VARIABLES\n",
        "\n",
        "GLOBAL_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "GLOBAL_SEED = 42\n",
        "PADDING_CONST = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nrkp8nHOpV9",
        "outputId": "24db23e1-891b-4539-fc2a-fb08dc85cb2b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  8 22:14:29 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYI4Wtuddst-"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtGHuZygfosD"
      },
      "source": [
        "def compute_integral_mc(intensity_network, hidden_states, src_padding_mask, time, events, alpha=-0.1, n_samples=100):\n",
        "    \"\"\"\n",
        "    Compute integral using Monte Carlo integration.\n",
        "    \"\"\"\n",
        "    # time differences (t_{j} - t{j-1})\n",
        "    dt = (time[:, 1:] - time[:, :-1]) * (~src_padding_mask[:, 1:])\n",
        "\n",
        "    # sample t from uniform distribution: \n",
        "    # since t \\in (t_{j-1}, t_j) which would lead to (t - t_{j-1}) / t_{j-1} \\in [0, (t_j - t_{j-1}) / t_{j-1}),\n",
        "    # we can reformulate this as (t_j - t_{j-1}) / t_{j-1} * u, where u \\in [0, 1)\n",
        "    current_influence = alpha * (dt.unsqueeze(2) / (time[:, :-1] + 1).unsqueeze(2)) * torch.rand([*dt.size(), n_samples], device=hidden_states.device)\n",
        "\n",
        "    # compute sum( lambda(u_i) ) / N\n",
        "    mc_intensity = intensity_network(hidden_states, events, current_inf=current_influence, mc_trick=True)\n",
        "\n",
        "    return mc_intensity * dt\n",
        "\n",
        "def compute_integral_li(lam, time, src_padding_mask):\n",
        "    dt = (time[:, 1:] - time[:, :-1]) * (~src_padding_mask[:, 1:])\n",
        "    dlam = (lam[:, 1:] + lam[:, :-1]) * (~src_padding_mask[:, 1:])\n",
        "\n",
        "    integral = dt * dlam\n",
        "    return 0.5 * integral"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbcIj2j2NJrb"
      },
      "source": [
        "class IntensityNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, n_event_types, device):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            hidden_size (int) - size of the hidden dimension (d_model),\n",
        "            n_event_types (int) - number of possible event types in the data\n",
        "        \"\"\"\n",
        "        super(IntensityNetwork, self).__init__()\n",
        "\n",
        "        self.n_events = n_event_types\n",
        "        self.device = device\n",
        "\n",
        "        # accounts for \"history\" and \"base\" (through bias) terms in eq.(6) of the paper\n",
        "        self.linear = nn.Linear(hidden_size, n_event_types)\n",
        "        self.softplus = nn.Softplus(threshold=10)\n",
        "\n",
        "    def generate_type_mask(self, events):\n",
        "        bs, ls = events.size()\n",
        "\n",
        "        type_mask = torch.zeros(bs, ls, self.n_events, device=self.device)\n",
        "        for k in range(self.n_events):\n",
        "            type_mask[:, :, k] = (events == k + 1).bool().to(self.device)\n",
        "        return type_mask\n",
        "    \n",
        "    def forward(self, hidden_states, events, current_inf=None, mc_trick=False):\n",
        "        intensity_terms = self.linear(hidden_states)\n",
        "        type_mask = self.generate_type_mask(events)\n",
        "\n",
        "        if mc_trick:\n",
        "            # this is a trick for Monte-Carlo integration, which allows to vectorize\n",
        "            # computation of (num_samples) intensity functions instead of making a loop\n",
        "\n",
        "            assert current_inf is not None, \"current influence cannot be None when mc_trick is True\"\n",
        "\n",
        "            intensity_terms = (intensity_terms[:, :-1, :] * type_mask[:, :-1, :]).sum(dim=2, keepdim=True)\n",
        "            continious_intensity = self.softplus( intensity_terms + current_inf )\n",
        "            conditional_lambda = continious_intensity.mean(dim=2)\n",
        "        else:\n",
        "            if current_inf is not None:\n",
        "                intensity_terms += current_inf\n",
        "            continious_intensity = self.softplus(intensity_terms)\n",
        "\n",
        "            # (continious_intensity * type_mask) gets type-specific instensity function (eq. (6))\n",
        "            # after summation along the 2nd dimension, conditional intensity function is obtained\n",
        "            conditional_lambda = (continious_intensity * type_mask).sum(dim=2)\n",
        "        \n",
        "        return conditional_lambda"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IzlD9rB_WlT"
      },
      "source": [
        "class HawkesTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, n_event_types, device, d_model=512, n_heads=8, dim_feedforward=2048, n_layers=6, dropout=0.1, activation='relu'):\n",
        "        \"\"\"\n",
        "        Input parameters:\n",
        "            n_event_types (int) - number of event types in the data,\n",
        "            d_model (int) - size of model's latent dimension,\n",
        "            n_heads (int) - number of heads in the Multihead Attention module,\n",
        "            dim_feedforward (int) - size of the feedforward network dimension,\n",
        "            n_layers (int) - number of Transformer encoder layers,\n",
        "            dropout (float) - dropout rate,\n",
        "            activation (string) - activation function for the feedforward network (relu or gelu)\n",
        "        \"\"\"\n",
        "        super(HawkesTransformer, self).__init__()\n",
        "\n",
        "        self.n_events = n_event_types\n",
        "        self.d_model = d_model\n",
        "        self.device = device\n",
        "\n",
        "        # initialize div term for temporal encoding\n",
        "        self.init_temporal_encoding()\n",
        "\n",
        "        # event type embedding\n",
        "        self.event_embedding = nn.Embedding(n_event_types + 1, d_model, padding_idx=PADDING_CONST)\n",
        "\n",
        "        # transformer encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward, dropout, activation)\n",
        "        self.transformer_layers = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "\n",
        "        # linear transformation of hidden states (\"history\" and \"base\" terms in eq.(6) of the THP paper) to\n",
        "        # type specific intensity\n",
        "        self.intensity_layer = IntensityNetwork(d_model, n_event_types, self.device)\n",
        "\n",
        "        # output prediction layers\n",
        "        self.time_predictor  = nn.Linear(d_model, 1, bias=False)\n",
        "        self.event_predictor = nn.Linear(d_model, n_event_types, bias=False)\n",
        "\n",
        "        # small constant\n",
        "        self.eps = torch.tensor([1e-9], device=self.device)\n",
        "\n",
        "    def generate_subsequent_mask(self, seq):\n",
        "        \"\"\"\n",
        "        Function to generate masking for the subsequent information in the sequences (masked self-attention).\n",
        "        Input:\n",
        "            seq (B, S) - batch of sequences.\n",
        "        \"\"\"\n",
        "        bs, ls = seq.size()\n",
        "        subsequent_mask = torch.triu( torch.ones(ls, ls, device=self.device, dtype=torch.bool), diagonal=1 )\n",
        "        \n",
        "        return subsequent_mask\n",
        "    \n",
        "    def generate_key_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        Masking the padded part of the sequence.\n",
        "        Input:\n",
        "            seq (B, S) - batch of sequences.\n",
        "        \"\"\"\n",
        "        padding_mask = seq.eq(PADDING_CONST)\n",
        "\n",
        "        return padding_mask\n",
        "\n",
        "    def init_temporal_encoding(self):\n",
        "        \"\"\"\n",
        "        Initializing the internal temporal encoding tensors.\n",
        "        \"\"\"\n",
        "        encoding_constant = torch.tensor(10000.0)\n",
        "\n",
        "        # for better numerical stability\n",
        "        self.te_div_term = torch.exp(2.0 * (torch.arange(0, self.d_model) // 2) * -torch.log(encoding_constant) / self.d_model).to(self.device)\n",
        "  \n",
        "    def temporal_encoding(self, t, non_padded_mask):\n",
        "        \"\"\"\n",
        "        Function to perform the temporal encoding on input timestamps.\n",
        "        Input:\n",
        "            t (B, S) - batch of timestamp sequences,\n",
        "            non_padded_mask (B, S) - binary mask indicating whether element is a padding (True) or not (False)\n",
        "        Output:\n",
        "            x (B, S, d_model) - raw model output,\n",
        "            lam (B, S, F) - intensity function,\n",
        "            time_pred (B, S) - timestamp prediction for the next event,\n",
        "            event_pred (B, S, n_event_types) - probabilities of event types\n",
        "        \"\"\"\n",
        "        temporal_enc = t.unsqueeze(-1) * self.te_div_term\n",
        "\n",
        "        temporal_enc[:, :, 0::2] = torch.sin(temporal_enc[:, :, 0::2])\n",
        "        temporal_enc[:, :, 1::2] = torch.cos(temporal_enc[:, :, 1::2])\n",
        "\n",
        "        return temporal_enc * non_padded_mask.unsqueeze(-1)\n",
        "    \n",
        "    def log_likelihood(self, hidden_states, cond_lam, time, events, alpha, integral='mc'):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            hidden_states (B, S, E) - hidden states of the network,\n",
        "            cond_lam (B, S, F) - conditional intensity function,\n",
        "            time (B, S) - ground truth for times,\n",
        "            events (B, S) - ground truth for event types,\n",
        "            alpha (int) - scaling constant,\n",
        "            integral (string) - method of integration: either Monte-Carlo or linear interpolation\n",
        "        \"\"\"\n",
        "        \n",
        "        src_padding_mask = self.generate_key_padding_mask(events)\n",
        "\n",
        "        # # compute event log-likelihood\n",
        "        event_part = cond_lam + self.eps\n",
        "        event_part.masked_fill_(src_padding_mask, 1.0)\n",
        "        event_part = event_part.log()\n",
        "        event_part = event_part.sum(dim=1)\n",
        "\n",
        "        # # compute non-event log-likelihood\n",
        "        if integral == 'mc':\n",
        "            non_event_part = compute_integral_mc(self.intensity_layer, hidden_states, src_padding_mask, time, events, alpha)\n",
        "        else:\n",
        "            non_event_part = compute_integral_li(cond_lam, time, src_padding_mask)\n",
        "        non_event_part = non_event_part.sum(dim=1)\n",
        "\n",
        "        # # compute total log-likelihood\n",
        "        log_likelihood = (event_part - non_event_part).sum()\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def time_error(self, time_pred, time):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            time_pred (B, S) - time predictions,\n",
        "            time (B, S) - ground truth for times\n",
        "        \"\"\"\n",
        "\n",
        "        time_ground_truth = time[:, 1:] - time[:, :-1]\n",
        "        time_pred = time_pred[:, :-1]\n",
        "\n",
        "        time_error = nn.MSELoss(reduction='sum')(time_pred, time_ground_truth)\n",
        "        return time_error\n",
        "\n",
        "    def event_error(self, event_pred, events):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            event_pred (B, S, K) - event probabilities predictions,\n",
        "            events (B, S) - ground truth for event types,\n",
        "        \"\"\"\n",
        "\n",
        "        event_ground_truth = events[:, 1:] - 1\n",
        "        event_pred = event_pred[:, :-1, :]\n",
        "\n",
        "        event_error = nn.CrossEntropyLoss(reduction='sum', ignore_index=-1)(event_pred.transpose(1, 2), event_ground_truth)\n",
        "        return event_error\n",
        "    \n",
        "    def forward(self, time, events):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            time (B, S) - input sequence of event timestamps,\n",
        "            events (B, S) - input sequence of event types\n",
        "        \"\"\"\n",
        "\n",
        "        # generate masks\n",
        "        src_key_padding_mask = self.generate_key_padding_mask(events)\n",
        "        src_non_padded_mask = ~src_key_padding_mask\n",
        "        src_mask = self.generate_subsequent_mask(events)\n",
        "\n",
        "        # perform encodings\n",
        "        temp_enc  = self.temporal_encoding(time, src_non_padded_mask)\n",
        "        event_enc = self.event_embedding(events)\n",
        "\n",
        "        # make pass through transformer encoder layers\n",
        "        x = event_enc + temp_enc\n",
        "        h = self.transformer_layers(x.permute(1, 0, 2), mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        h = h.permute(1, 0, 2)\n",
        "\n",
        "        # obtain conditional intensity function\n",
        "        cond_lam = self.intensity_layer(h, events)\n",
        "\n",
        "        # make predictions\n",
        "        time_pred  = self.time_predictor(h).squeeze(2) * src_non_padded_mask\n",
        "        event_pred = self.event_predictor(h) * src_non_padded_mask.unsqueeze(-1)\n",
        "\n",
        "        return h, cond_lam, time_pred, event_pred"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d100mIrBR34l"
      },
      "source": [
        "def run_epoch(model, dataloader, device, optimizer=None):\n",
        "    \n",
        "    ll_loss_epoch, tp_loss_epoch, ec_loss_epoch = 0., 0., 0.\n",
        "    event_num_total, event_pred_correct, event_pred_total = 0, 0, 0\n",
        "\n",
        "    with torch.set_grad_enabled(optimizer is not None):\n",
        "        for time, events in dataloader:\n",
        "            time = time.to(device)\n",
        "            events = events.to(device)\n",
        "\n",
        "            h, cond_lam, time_pred, event_pred = model(time, events)\n",
        "\n",
        "            # Log-likelihood (loss for the whole sequence)\n",
        "            loss_ll = model.log_likelihood(h, cond_lam, time, events, -0.1, 'mc')\n",
        "\n",
        "            # Time prediction loss\n",
        "            loss_tp = model.time_error(time_pred, time)\n",
        "\n",
        "            # Event type classficiation loss\n",
        "            loss_ec = model.event_error(event_pred, events)\n",
        "\n",
        "            # Combined loss function\n",
        "            scale = 0.01\n",
        "            loss = -loss_ll + loss_ec + loss_tp * scale\n",
        "\n",
        "            if optimizer is not None:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            ll_loss_epoch += loss_ll.item()\n",
        "            tp_loss_epoch += loss_tp.item()\n",
        "            ec_loss_epoch += loss_ec.item()\n",
        "\n",
        "            batch_num_events = events.ne(PADDING_CONST).sum().item()\n",
        "            event_num_total += batch_num_events\n",
        "            event_pred_correct += (event_pred[:, :-1, :].argmax(dim=2) == events[:, 1:] - 1).sum().item()\n",
        "            event_pred_total += batch_num_events - events.shape[0]\n",
        "    \n",
        "    ll_loss_epoch /= event_num_total\n",
        "    tp_loss_epoch /= event_pred_total\n",
        "    ec_loss_epoch /= event_pred_total\n",
        "    accuracy = event_pred_correct / event_pred_total\n",
        "\n",
        "    return ll_loss_epoch, tp_loss_epoch, ec_loss_epoch, accuracy"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Chfl0K2QRnk"
      },
      "source": [
        "import time as t\n",
        "\n",
        "def train(model, n_epochs, optimizer, train_loader, val_loader, scheduler=None, device=None, verbose=True, freq=None):\n",
        "    if verbose and freq is None:\n",
        "        freq = max(n_epochs // 10, 1)\n",
        "    if device is None:\n",
        "        device = GLOBAL_DEVICE\n",
        "\n",
        "    train_loss_ll_history, train_loss_tp_history, train_loss_ec_history, train_accuracy_history = [], [], [], []\n",
        "    val_loss_ll_history, val_loss_tp_history, val_loss_ec_history, val_accuracy_history = [], [], [], []\n",
        "\n",
        "    time_start = t.time()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        model.train()\n",
        "        train_loss_ll, train_loss_tp, train_loss_ec, train_accuracy = run_epoch(model, train_loader, device, optimizer)\n",
        "        train_loss_ll_history.append( train_loss_ll )\n",
        "        train_loss_tp_history.append( train_loss_tp )\n",
        "        train_loss_ec_history.append( train_loss_ec )\n",
        "        train_accuracy_history.append( train_accuracy )\n",
        "\n",
        "        model.eval()\n",
        "        val_loss_ll, val_loss_tp, val_loss_ec, val_accuracy = run_epoch(model, val_loader, device)\n",
        "        val_loss_ll_history.append( val_loss_ll )\n",
        "        val_loss_tp_history.append( val_loss_tp )\n",
        "        val_loss_ec_history.append( val_loss_ec )\n",
        "        val_accuracy_history.append( val_accuracy )\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        time_epoch_end = t.time() - time_start\n",
        "\n",
        "        if verbose and epoch % freq == 0:\n",
        "            print(\"[ Epoch {} ]\".format(epoch))\n",
        "            print(\"(Training)     log-likelihood: {}, RMSE: {}, CE: {}, accuracy: {}\".format( train_loss_ll, np.sqrt(train_loss_tp), train_loss_ec, train_accuracy ))\n",
        "            print(\"(Validation)   log-likelihood: {}, RMSE: {}, CE: {}, accuracy: {}\".format( val_loss_ll, np.sqrt(val_loss_tp), val_loss_ec, val_accuracy ))\n",
        "            print(\"Time elapsed: {:.2f} s\".format(time_epoch_end))\n",
        "\n",
        "    train_history = {\n",
        "        'log-likelihood' : train_loss_ll_history,\n",
        "        'time mse' : train_loss_tp_history,\n",
        "        'cross entropy' : train_loss_ec_history,\n",
        "        'accuracy' : train_accuracy_history\n",
        "    }\n",
        "\n",
        "    val_history = {\n",
        "        'log-likelihood' : val_loss_ll_history,\n",
        "        'time mse' : val_loss_tp_history,\n",
        "        'cross entropy' : val_loss_ec_history,\n",
        "        'accuracy' : val_accuracy_history\n",
        "    }\n",
        "    return train_history, val_history"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcW8ryPzVL7D"
      },
      "source": [
        "#!unzip -q fin_data.zip -d data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov7o0hyHWUG1"
      },
      "source": [
        "import pickle\n",
        "\n",
        "class NHPDataset(utils_data.Dataset):\n",
        "    ''' \n",
        "    Create Dataset for Neural Hawkey Process\n",
        "    '''\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.event_type = []\n",
        "        self.event_time = []\n",
        "\n",
        "        with open(file_path, 'rb') as f:\n",
        "\n",
        "            if 'dev' in file_path:\n",
        "                seqs = pickle.load(f, encoding='latin1')['dev']\n",
        "            elif 'train' in file_path:\n",
        "                seqs = pickle.load(f, encoding='latin1')['train']\n",
        "            elif 'test' in file_path:\n",
        "                seqs = pickle.load(f, encoding='latin1')['test']\n",
        "\n",
        "            for idx, seq in enumerate(seqs):\n",
        "                self.event_type.append(torch.Tensor([int(event['type_event']) for event in seq]))\n",
        "                self.event_time.append(torch.Tensor([float(event['time_since_start']) for event in seq]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.event_type)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        event_type = torch.LongTensor(self.event_type[index].long() + 1)\n",
        "        event_time = torch.Tensor(self.event_time[index])\n",
        "        seq_len = torch.tensor(len(event_type))\n",
        "        event_last_time = torch.sum(event_time)\n",
        "\n",
        "        X = torch.stack((event_time, event_type), dim=1)\n",
        "        \n",
        "        return event_time, event_type"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttnAtcwBXuLm"
      },
      "source": [
        "train_dataset = NHPDataset('data/train.pkl')\n",
        "val_dataset = NHPDataset('data/test.pkl')\n",
        "\n",
        "train_loader = utils_data.DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "val_loader = utils_data.DataLoader(val_dataset, batch_size=5, shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn1UweNSaayy",
        "outputId": "a50f5441-b0e5-4159-f2af-9529fce08e99"
      },
      "source": [
        "model = HawkesTransformer(2, GLOBAL_DEVICE, 512, 4, 1024, 4, 0.1, 'relu').to(GLOBAL_DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-05)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5)\n",
        "\n",
        "train_history, val_history = train(model, 50, optimizer, train_loader, val_loader, scheduler, GLOBAL_DEVICE, freq=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Epoch 1 ]\n",
            "(Training)     log-likelihood: -1.268742206069432, RMSE: 20.24977521151604, CE: 0.768273220224491, accuracy: 0.5478601567209163\n",
            "(Validation)   log-likelihood: -1.3759121199770055, RMSE: 17.456689786167843, CE: 0.6703899379859224, accuracy: 0.6134782608695653\n",
            "Time elapsed: 10.79 s\n",
            "[ Epoch 2 ]\n",
            "(Training)     log-likelihood: -1.2351691668706688, RMSE: 20.2477570646395, CE: 0.6827322709201912, accuracy: 0.5839762909383163\n",
            "(Validation)   log-likelihood: -1.292960654190855, RMSE: 17.459266752290223, CE: 0.6656653598656401, accuracy: 0.6160869565217392\n",
            "Time elapsed: 21.54 s\n",
            "[ Epoch 3 ]\n",
            "(Training)     log-likelihood: -1.1781854846935573, RMSE: 20.23963673672891, CE: 0.6626432729880366, accuracy: 0.6203301855200589\n",
            "(Validation)   log-likelihood: -1.2988057959985675, RMSE: 17.46235149840766, CE: 0.6671821277268267, accuracy: 0.6152777777777778\n",
            "Time elapsed: 32.30 s\n",
            "[ Epoch 4 ]\n",
            "(Training)     log-likelihood: -1.1237578227001104, RMSE: 20.233397889223127, CE: 0.6654452964103627, accuracy: 0.6143560377737592\n",
            "(Validation)   log-likelihood: -1.2733211577154329, RMSE: 17.46646375009672, CE: 0.672049657849298, accuracy: 0.6078502415458937\n",
            "Time elapsed: 43.07 s\n",
            "[ Epoch 5 ]\n",
            "(Training)     log-likelihood: -1.0585089746587408, RMSE: 20.221327657735493, CE: 0.6676578570015823, accuracy: 0.6139140044203335\n",
            "(Validation)   log-likelihood: -1.2583838155675136, RMSE: 17.468126386453843, CE: 0.6677367485894097, accuracy: 0.6146135265700483\n",
            "Time elapsed: 53.82 s\n",
            "[ Epoch 6 ]\n",
            "(Training)     log-likelihood: -1.0451257300925227, RMSE: 20.2045046075856, CE: 0.6651189253116419, accuracy: 0.6139340968454893\n",
            "(Validation)   log-likelihood: -1.2683007827225008, RMSE: 17.47723918792455, CE: 0.6687475173139342, accuracy: 0.614927536231884\n",
            "Time elapsed: 64.59 s\n",
            "[ Epoch 7 ]\n",
            "(Training)     log-likelihood: -1.0175935886290215, RMSE: 20.186903560494905, CE: 0.6648452248834221, accuracy: 0.6125711606724265\n",
            "(Validation)   log-likelihood: -1.1650470817391059, RMSE: 17.47301673963928, CE: 0.661767415954295, accuracy: 0.6164251207729469\n",
            "Time elapsed: 75.35 s\n",
            "[ Epoch 8 ]\n",
            "(Training)     log-likelihood: -1.0128346780478976, RMSE: 20.168873726949762, CE: 0.6602211644920802, accuracy: 0.6193121693121693\n",
            "(Validation)   log-likelihood: -1.1567601773362108, RMSE: 17.479006126146132, CE: 0.6628764070520079, accuracy: 0.6152898550724638\n",
            "Time elapsed: 86.11 s\n",
            "[ Epoch 9 ]\n",
            "(Training)     log-likelihood: -0.998875758208212, RMSE: 20.14824703350554, CE: 0.662078045990724, accuracy: 0.6173397629093832\n",
            "(Validation)   log-likelihood: -1.2024623452107208, RMSE: 17.48580939107885, CE: 0.660233413770003, accuracy: 0.6156763285024155\n",
            "Time elapsed: 96.88 s\n",
            "[ Epoch 10 ]\n",
            "(Training)     log-likelihood: -0.9972494977099444, RMSE: 20.12926504603206, CE: 0.6599535303803747, accuracy: 0.6165360659031545\n",
            "(Validation)   log-likelihood: -1.2419132209658192, RMSE: 17.484201831890815, CE: 0.6713499649711278, accuracy: 0.5989855072463768\n",
            "Time elapsed: 107.63 s\n",
            "[ Epoch 11 ]\n",
            "(Training)     log-likelihood: -0.9841065442078019, RMSE: 20.11597587339, CE: 0.6605154609641852, accuracy: 0.6174000401848503\n",
            "(Validation)   log-likelihood: -1.2176826651910246, RMSE: 17.481456763616713, CE: 0.6633375143889644, accuracy: 0.6143840579710145\n",
            "Time elapsed: 118.39 s\n",
            "[ Epoch 12 ]\n",
            "(Training)     log-likelihood: -0.9731616098965133, RMSE: 20.11022349879901, CE: 0.6580624682131555, accuracy: 0.6213950840533119\n",
            "(Validation)   log-likelihood: -1.2110820576077634, RMSE: 17.48373571330663, CE: 0.6598103634746755, accuracy: 0.6163647342995169\n",
            "Time elapsed: 129.15 s\n",
            "[ Epoch 13 ]\n",
            "(Training)     log-likelihood: -0.9706252445411101, RMSE: 20.10317526517781, CE: 0.657167560622614, accuracy: 0.6212644832897998\n",
            "(Validation)   log-likelihood: -1.1949742768843297, RMSE: 17.480442639493493, CE: 0.6598835555366848, accuracy: 0.6158937198067633\n",
            "Time elapsed: 139.91 s\n",
            "[ Epoch 14 ]\n",
            "(Training)     log-likelihood: -0.9689159348741254, RMSE: 20.099009332392328, CE: 0.6570971095947609, accuracy: 0.620397160270578\n",
            "(Validation)   log-likelihood: -1.2218978134541805, RMSE: 17.476710824826945, CE: 0.6597427353421271, accuracy: 0.6147946859903382\n",
            "Time elapsed: 150.68 s\n",
            "[ Epoch 15 ]\n",
            "(Training)     log-likelihood: -0.9632075901637458, RMSE: 20.09403856906952, CE: 0.6575159634056242, accuracy: 0.6208559373116335\n",
            "(Validation)   log-likelihood: -1.2234991167940479, RMSE: 17.479852311395554, CE: 0.6606291250330238, accuracy: 0.6141666666666666\n",
            "Time elapsed: 161.44 s\n",
            "[ Epoch 16 ]\n",
            "(Training)     log-likelihood: -0.9619083347456563, RMSE: 20.090879730663925, CE: 0.6570872268947575, accuracy: 0.6208191011988481\n",
            "(Validation)   log-likelihood: -1.1942366011266399, RMSE: 17.476930623821037, CE: 0.6615276686811217, accuracy: 0.6123671497584541\n",
            "Time elapsed: 172.19 s\n",
            "[ Epoch 17 ]\n",
            "(Training)     log-likelihood: -0.9536962944567892, RMSE: 20.08650140253897, CE: 0.6567568693594702, accuracy: 0.6201225637934499\n",
            "(Validation)   log-likelihood: -1.2226624322944153, RMSE: 17.474949362002718, CE: 0.6604845690151344, accuracy: 0.6152777777777778\n",
            "Time elapsed: 182.95 s\n",
            "[ Epoch 18 ]\n",
            "(Training)     log-likelihood: -0.9493299181271132, RMSE: 20.083895553179175, CE: 0.6569379856977513, accuracy: 0.619332261737325\n",
            "(Validation)   log-likelihood: -1.2511794252476158, RMSE: 17.477017443866245, CE: 0.6597886119602959, accuracy: 0.6152777777777778\n",
            "Time elapsed: 193.71 s\n",
            "[ Epoch 19 ]\n",
            "(Training)     log-likelihood: -0.9496399622543604, RMSE: 20.08019862738064, CE: 0.6565807270737057, accuracy: 0.6209430044873083\n",
            "(Validation)   log-likelihood: -1.2519721865222595, RMSE: 17.4795588540583, CE: 0.6600534544129303, accuracy: 0.6122826086956522\n",
            "Time elapsed: 204.47 s\n",
            "[ Epoch 20 ]\n",
            "(Training)     log-likelihood: -0.9364428598965133, RMSE: 20.07724981777268, CE: 0.6559323179594887, accuracy: 0.6210870002009242\n",
            "(Validation)   log-likelihood: -1.2818554039599668, RMSE: 17.47562666816788, CE: 0.6599041865989206, accuracy: 0.6143840579710145\n",
            "Time elapsed: 215.24 s\n",
            "[ Epoch 21 ]\n",
            "(Training)     log-likelihood: -0.933458712960522, RMSE: 20.07451396590264, CE: 0.6559891254981247, accuracy: 0.6202464670819101\n",
            "(Validation)   log-likelihood: -1.2598720139157211, RMSE: 17.47555127873125, CE: 0.660042509364621, accuracy: 0.6144082125603865\n",
            "Time elapsed: 226.01 s\n",
            "[ Epoch 22 ]\n",
            "(Training)     log-likelihood: -0.9278462623656724, RMSE: 20.073298542604437, CE: 0.6560914451267497, accuracy: 0.6206181769472908\n",
            "(Validation)   log-likelihood: -1.2503802335621796, RMSE: 17.477039664726554, CE: 0.6592027658656023, accuracy: 0.6144685990338165\n",
            "Time elapsed: 236.76 s\n",
            "[ Epoch 23 ]\n",
            "(Training)     log-likelihood: -0.938820674015454, RMSE: 20.073895713524074, CE: 0.6561610262677064, accuracy: 0.6202498158194361\n",
            "(Validation)   log-likelihood: -1.226116935260621, RMSE: 17.474458755105207, CE: 0.6624803920874849, accuracy: 0.6126690821256039\n",
            "Time elapsed: 247.52 s\n",
            "[ Epoch 24 ]\n",
            "(Training)     log-likelihood: -0.9270050311391065, RMSE: 20.071057598212604, CE: 0.6562900311484914, accuracy: 0.620186189806443\n",
            "(Validation)   log-likelihood: -1.241997294868865, RMSE: 17.475440382003256, CE: 0.6591525283297479, accuracy: 0.6146497584541063\n",
            "Time elapsed: 258.28 s\n",
            "[ Epoch 25 ]\n",
            "(Training)     log-likelihood: -0.9171812753171973, RMSE: 20.068278563253287, CE: 0.6553924779244943, accuracy: 0.6197106690777577\n",
            "(Validation)   log-likelihood: -1.22224158200769, RMSE: 17.477733319610774, CE: 0.6592712254915837, accuracy: 0.6133816425120773\n",
            "Time elapsed: 269.03 s\n",
            "[ Epoch 26 ]\n",
            "(Training)     log-likelihood: -0.9149963429988534, RMSE: 20.06793844394508, CE: 0.6553121213048775, accuracy: 0.6218873484696269\n",
            "(Validation)   log-likelihood: -1.2354971589570454, RMSE: 17.477791436926463, CE: 0.6599884475486866, accuracy: 0.6131280193236714\n",
            "Time elapsed: 279.79 s\n",
            "[ Epoch 27 ]\n",
            "(Training)     log-likelihood: -0.9150763582885575, RMSE: 20.06722517334736, CE: 0.655440570242909, accuracy: 0.6199115933293149\n",
            "(Validation)   log-likelihood: -1.2362349105485242, RMSE: 17.473041804958225, CE: 0.660271989757888, accuracy: 0.6142512077294686\n",
            "Time elapsed: 290.54 s\n",
            "[ Epoch 28 ]\n",
            "(Training)     log-likelihood: -0.903256820081559, RMSE: 20.064835224155743, CE: 0.655286907665888, accuracy: 0.620899470899471\n",
            "(Validation)   log-likelihood: -1.2550374758804377, RMSE: 17.475430247651758, CE: 0.6595447661911232, accuracy: 0.6122584541062802\n",
            "Time elapsed: 301.30 s\n",
            "[ Epoch 29 ]\n",
            "(Training)     log-likelihood: -0.9063437102457232, RMSE: 20.06501935668763, CE: 0.6552275362500837, accuracy: 0.6204574375460451\n",
            "(Validation)   log-likelihood: -1.1882298994812086, RMSE: 17.4752932335582, CE: 0.6596479362450935, accuracy: 0.615048309178744\n",
            "Time elapsed: 312.05 s\n",
            "[ Epoch 30 ]\n",
            "(Training)     log-likelihood: -0.9070250547405929, RMSE: 20.06274558692639, CE: 0.6549278471332713, accuracy: 0.620045542830353\n",
            "(Validation)   log-likelihood: -1.3027864455716602, RMSE: 17.47323725570296, CE: 0.6595003137266002, accuracy: 0.6136835748792271\n",
            "Time elapsed: 322.82 s\n",
            "[ Epoch 31 ]\n",
            "(Training)     log-likelihood: -0.8959558883145442, RMSE: 20.06155178086633, CE: 0.6546707824743822, accuracy: 0.6214051302658897\n",
            "(Validation)   log-likelihood: -1.2611243286869063, RMSE: 17.47471165612502, CE: 0.6595244417789478, accuracy: 0.6139855072463768\n",
            "Time elapsed: 333.57 s\n",
            "[ Epoch 32 ]\n",
            "(Training)     log-likelihood: -0.8876803752228332, RMSE: 20.060274079700886, CE: 0.6545927477333233, accuracy: 0.6211171388386578\n",
            "(Validation)   log-likelihood: -1.2131229171476836, RMSE: 17.474569124187624, CE: 0.6594881361809329, accuracy: 0.6138526570048309\n",
            "Time elapsed: 344.33 s\n",
            "[ Epoch 33 ]\n",
            "(Training)     log-likelihood: -0.8866299567306083, RMSE: 20.061252195904416, CE: 0.6546087229118529, accuracy: 0.6211941598017547\n",
            "(Validation)   log-likelihood: -1.2050846098933203, RMSE: 17.475175449050848, CE: 0.6595297491838391, accuracy: 0.6143961352657005\n",
            "Time elapsed: 355.09 s\n",
            "[ Epoch 34 ]\n",
            "(Training)     log-likelihood: -0.8795121463700161, RMSE: 20.05915672422757, CE: 0.6541727787667019, accuracy: 0.6214386176411493\n",
            "(Validation)   log-likelihood: -1.2497992385109602, RMSE: 17.475626670867467, CE: 0.6592097215145682, accuracy: 0.6132971014492754\n",
            "Time elapsed: 365.85 s\n",
            "[ Epoch 35 ]\n",
            "(Training)     log-likelihood: -0.8800791090491196, RMSE: 20.059580105032857, CE: 0.6542491751117642, accuracy: 0.6207923112986404\n",
            "(Validation)   log-likelihood: -1.2761890104912734, RMSE: 17.476562296192277, CE: 0.6594256090541969, accuracy: 0.613731884057971\n",
            "Time elapsed: 376.60 s\n",
            "[ Epoch 36 ]\n",
            "(Training)     log-likelihood: -0.8789304458389826, RMSE: 20.058443997621126, CE: 0.6542441945187446, accuracy: 0.6211104413636059\n",
            "(Validation)   log-likelihood: -1.2405792273140643, RMSE: 17.474955145073526, CE: 0.6594453319604846, accuracy: 0.6119202898550725\n",
            "Time elapsed: 387.36 s\n",
            "[ Epoch 37 ]\n",
            "(Training)     log-likelihood: -0.8767981617831676, RMSE: 20.057641183073383, CE: 0.6541459300019674, accuracy: 0.620899470899471\n",
            "(Validation)   log-likelihood: -1.2689536277117575, RMSE: 17.475770417418783, CE: 0.6592834561112998, accuracy: 0.6134178743961353\n",
            "Time elapsed: 398.12 s\n",
            "[ Epoch 38 ]\n",
            "(Training)     log-likelihood: -0.8759933665281544, RMSE: 20.056479006971205, CE: 0.6542662327435369, accuracy: 0.6212142522269104\n",
            "(Validation)   log-likelihood: -1.2499577149321, RMSE: 17.47232633978917, CE: 0.6593174942680027, accuracy: 0.6124516908212561\n",
            "Time elapsed: 408.87 s\n",
            "[ Epoch 39 ]\n",
            "(Training)     log-likelihood: -0.8725143703864116, RMSE: 20.056461062424788, CE: 0.6543593041009477, accuracy: 0.6200723327305606\n",
            "(Validation)   log-likelihood: -1.217380478586305, RMSE: 17.47494034605079, CE: 0.6597044895927687, accuracy: 0.6142149758454106\n",
            "Time elapsed: 419.63 s\n",
            "[ Epoch 40 ]\n",
            "(Training)     log-likelihood: -0.8718370830508687, RMSE: 20.055440125745356, CE: 0.6539166134269724, accuracy: 0.6211338825262875\n",
            "(Validation)   log-likelihood: -1.210933072190233, RMSE: 17.472626560380554, CE: 0.65938932409609, accuracy: 0.6135869565217391\n",
            "Time elapsed: 430.39 s\n",
            "[ Epoch 41 ]\n",
            "(Training)     log-likelihood: -0.8694286963892655, RMSE: 20.05530989210679, CE: 0.6539071460489083, accuracy: 0.6211673699015471\n",
            "(Validation)   log-likelihood: -1.2265730990604267, RMSE: 17.47434060353963, CE: 0.6593307156032986, accuracy: 0.6131521739130434\n",
            "Time elapsed: 441.15 s\n",
            "[ Epoch 42 ]\n",
            "(Training)     log-likelihood: -0.8650118000893425, RMSE: 20.054300975735064, CE: 0.6538544295948865, accuracy: 0.621318063090215\n",
            "(Validation)   log-likelihood: -1.1988627965027518, RMSE: 17.472796399331518, CE: 0.6595511822539252, accuracy: 0.613768115942029\n",
            "Time elapsed: 451.91 s\n",
            "[ Epoch 43 ]\n",
            "(Training)     log-likelihood: -0.8667394895707794, RMSE: 20.054551992774662, CE: 0.6537457100840952, accuracy: 0.6211338825262875\n",
            "(Validation)   log-likelihood: -1.2151109676568155, RMSE: 17.472804004278956, CE: 0.6594372086824426, accuracy: 0.613816425120773\n",
            "Time elapsed: 462.66 s\n",
            "[ Epoch 44 ]\n",
            "(Training)     log-likelihood: -0.8623025363228549, RMSE: 20.054526691812544, CE: 0.6537288846401782, accuracy: 0.6211908110642288\n",
            "(Validation)   log-likelihood: -1.2318884321518773, RMSE: 17.473402001378915, CE: 0.6594603814019098, accuracy: 0.6130434782608696\n",
            "Time elapsed: 473.41 s\n",
            "[ Epoch 45 ]\n",
            "(Training)     log-likelihood: -0.8602283144343594, RMSE: 20.053036865520628, CE: 0.6538342357919346, accuracy: 0.6211271850512357\n",
            "(Validation)   log-likelihood: -1.2333305736215037, RMSE: 17.47390694614628, CE: 0.6592822177168252, accuracy: 0.6132487922705314\n",
            "Time elapsed: 484.17 s\n",
            "[ Epoch 46 ]\n",
            "(Training)     log-likelihood: -0.8599170263513316, RMSE: 20.053643615466196, CE: 0.6539040262289867, accuracy: 0.6212142522269104\n",
            "(Validation)   log-likelihood: -1.218370765529582, RMSE: 17.47376245631213, CE: 0.6594944578898702, accuracy: 0.6130676328502416\n",
            "Time elapsed: 494.93 s\n",
            "[ Epoch 47 ]\n",
            "(Training)     log-likelihood: -0.8603661826583475, RMSE: 20.052392894872987, CE: 0.653736429110366, accuracy: 0.6213615966780524\n",
            "(Validation)   log-likelihood: -1.255123470660717, RMSE: 17.473629956187388, CE: 0.6593354421422102, accuracy: 0.6133937198067633\n",
            "Time elapsed: 505.68 s\n",
            "[ Epoch 48 ]\n",
            "(Training)     log-likelihood: -0.8623510277134386, RMSE: 20.052469983212177, CE: 0.6539371244443188, accuracy: 0.6205913870470833\n",
            "(Validation)   log-likelihood: -1.2376244189512025, RMSE: 17.473172759088865, CE: 0.659420587659458, accuracy: 0.6133212560386473\n",
            "Time elapsed: 516.43 s\n",
            "[ Epoch 49 ]\n",
            "(Training)     log-likelihood: -0.8561585520864718, RMSE: 20.05221877963226, CE: 0.6536470465966362, accuracy: 0.6205545509342978\n",
            "(Validation)   log-likelihood: -1.249966374709034, RMSE: 17.473651078788027, CE: 0.6593336140360809, accuracy: 0.6132729468599034\n",
            "Time elapsed: 527.19 s\n",
            "[ Epoch 50 ]\n",
            "(Training)     log-likelihood: -0.8567859079196796, RMSE: 20.052072443301178, CE: 0.6537235148872312, accuracy: 0.6207923112986404\n",
            "(Validation)   log-likelihood: -1.2343824655788225, RMSE: 17.472815791215314, CE: 0.6593324169214221, accuracy: 0.6134903381642512\n",
            "Time elapsed: 537.96 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMNXC3S9eLb"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}