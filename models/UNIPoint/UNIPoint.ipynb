{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIPoint",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Yfy2B76Z9Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgRk_PwDD4ct"
      },
      "source": [
        "# UNIPoint model\n",
        "class UNIPoint(nn.Module):\n",
        "    def __init__(self, batch_size, n_inputs, n_neurons, n_parameters, n_basis_functions):\n",
        "      \"\"\"\n",
        "      Input parameters:\n",
        "      n_neurons - number of neurons inside RNN\n",
        "      n_parameters - expecteed number of parameters in basis function\n",
        "      n_basis_functions - number of basis functions\n",
        "      \"\"\"\n",
        "      super(UNIPoint, self).__init__()\n",
        "        \n",
        "      self.rnn = nn.RNNCell(n_inputs, n_neurons)\n",
        "      self.hx = torch.randn(batch_size, n_neurons) # initialize hidden state \n",
        "      self.h2p = nn.Linear(n_neurons, n_parameters)\n",
        "      self.parameters = torch.randn(batch_size, n_parameters) # initialize parameters for intensity functions\n",
        "      self.basis_res = torch.randn(batch_size, n_parameters)\n",
        "      self.Softplus = torch.nn.Softplus(beta = 1)\n",
        "      self.result = torch.randn(1)\n",
        "\n",
        "    def ReLU(self, parameters, time):\n",
        "      \"\"\"Function to apply Rectified Linear Unit (ReLU) as basis function inside network \n",
        "        Input parameters:\n",
        "          parameters - alpha, beta for basis function's value calculation\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "        \n",
        "      self.output = torch.relu(self.parameters[:,0] @ time + self.parameters[:,1]) \n",
        "      return self.output\n",
        "        \n",
        "    def forward(self, X, time):\n",
        "      \"\"\"Input parameters:\n",
        "          X - batch with data\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "        \n",
        "      output = []\n",
        "\n",
        "      # for each time step\n",
        "      for i in range(2):\n",
        "          self.hx = self.rnn(X[i], self.hx)\n",
        "          self.parameters = self.h2p(self.hx)\n",
        "          self.basis_res = self.ReLU(self.parameters, time)\n",
        "          self.result = self.Softplus(torch.sum(self.basis_res))\n",
        "\n",
        "          output.append(self.hx)\n",
        "\n",
        "        \n",
        "      return output, self.hx, self.parameters, self.basis_res, self.result"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4NRCEyD4a4",
        "outputId": "41adc32f-67cc-4836-bf3c-0d34f4b4d9f2"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "FIXED_BATCH_SIZE = 4 # our batch size is fixed for now\n",
        "N_INPUT = 3\n",
        "N_NEURONS = 1\n",
        "\n",
        "N_PARAMETERS = 2\n",
        "N_BASIS_FUNCTIONS = 8\n",
        "\n",
        "# for now we generate random values for time in [0,T]\n",
        "TIME = torch.from_numpy(np.sort(np.random.sample(FIXED_BATCH_SIZE))).type(torch.float) # TIME size is (FIXED_BATCH_SIZE, 1)\n",
        "\n",
        "X_batch = torch.tensor([[[0,1,2], [3,4,5], \n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        [[9,8,7], [0,0,0], \n",
        "                         [6,5,4], [3,2,1]]], dtype = torch.float) # X0 and X1\n",
        "\n",
        "\n",
        "model = UNIPoint(FIXED_BATCH_SIZE, N_INPUT, N_NEURONS, N_PARAMETERS, N_BASIS_FUNCTIONS)\n",
        "print(model)\n",
        "output_val, states_val, params_value, basis_value, res = model(X_batch, TIME)\n",
        "print()\n",
        "print(output_val) # contains all output for all timesteps\n",
        "print()\n",
        "print(states_val) # contains values for final state or final timestep, i.e., t=1\n",
        "print()\n",
        "print(params_value) # contains values for parameters for final timesteps\n",
        "print()\n",
        "print(basis_value) # contains values of basis functions (now it is only ReLU functions)\n",
        "print()\n",
        "print(res) # contains values of SoftPlus activation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIPoint(\n",
            "  (rnn): RNNCell(3, 1)\n",
            "  (h2p): Linear(in_features=1, out_features=2, bias=True)\n",
            "  (Softplus): Softplus(beta=1, threshold=20)\n",
            ")\n",
            "\n",
            "[tensor([[ 0.9483],\n",
            "        [ 0.9587],\n",
            "        [ 0.9964],\n",
            "        [-0.9997]], grad_fn=<TanhBackward>), tensor([[ 0.3097],\n",
            "        [-0.5498],\n",
            "        [-0.4934],\n",
            "        [-0.4835]], grad_fn=<TanhBackward>)]\n",
            "\n",
            "tensor([[ 0.3097],\n",
            "        [-0.5498],\n",
            "        [-0.4934],\n",
            "        [-0.4835]], grad_fn=<TanhBackward>)\n",
            "\n",
            "tensor([[0.7549, 0.7445],\n",
            "        [0.4141, 0.7111],\n",
            "        [0.4364, 0.7133],\n",
            "        [0.4403, 0.7137]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "tensor([2.2191, 2.1857, 2.1879, 2.1883], grad_fn=<ReluBackward0>)\n",
            "\n",
            "tensor(8.7811, grad_fn=<SoftplusBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1s0915AnUEM"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}
