{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIPoint",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Yfy2B76Z9Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgRk_PwDD4ct"
      },
      "source": [
        "# UNIPoint model\n",
        "class UNIPoint(nn.Module):\n",
        "    def __init__(self, batch_size, n_inputs, n_neurons, n_parameters, n_basis_functions):\n",
        "        super(UNIPoint, self).__init__()\n",
        "        \n",
        "        self.rnn = nn.RNNCell(n_inputs, n_neurons)\n",
        "        self.hx = torch.randn(batch_size, n_neurons) # initialize hidden state \n",
        "        self.h2p = nn.Linear(n_neurons, n_parameters)\n",
        "        self.parameters = torch.randn(batch_size, n_parameters) # initialize parameters for intensity functions\n",
        "        self.basis_res = torch.randn(batch_size, n_parameters)\n",
        "        self.Softplus = torch.nn.Softplus(beta = 1)\n",
        "        self.result = torch.randn(1)\n",
        "\n",
        "    def ReLU(self, parameters, time):\n",
        "        self.output = np.maximum( (self.parameters[:,0] @ time + self.parameters[:,1]).detach().numpy(), 0 ) \n",
        "        return torch.from_numpy(self.output)\n",
        "        \n",
        "    def forward(self, X, time):\n",
        "        output = []\n",
        "\n",
        "        # for each time step\n",
        "        for i in range(2):\n",
        "            self.hx = self.rnn(X[i], self.hx)\n",
        "            self.parameters = self.h2p(self.hx)\n",
        "            self.basis_res = self.ReLU(self.parameters, time)\n",
        "            self.result = self.Softplus(torch.sum(self.basis_res))\n",
        "\n",
        "            output.append(self.hx)\n",
        "\n",
        "        \n",
        "        return output, self.hx, self.parameters, self.basis_res, self.result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4NRCEyD4a4",
        "outputId": "f4e9fd19-7b0d-4f19-f658-55f5dedffed7"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "FIXED_BATCH_SIZE = 4 # our batch size is fixed for now\n",
        "N_INPUT = 3\n",
        "N_NEURONS = 1\n",
        "\n",
        "N_PARAMETERS = 2\n",
        "N_BASIS_FUNCTIONS = 8\n",
        "\n",
        "# for now we generate random values for time in [0,T]\n",
        "TIME = torch.from_numpy(np.sort(np.random.sample(FIXED_BATCH_SIZE))).type(torch.float) # TIME size is (FIXED_BATCH_SIZE, 1)\n",
        "\n",
        "X_batch = torch.tensor([[[0,1,2], [3,4,5], \n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        [[9,8,7], [0,0,0], \n",
        "                         [6,5,4], [3,2,1]]], dtype = torch.float) # X0 and X1\n",
        "\n",
        "\n",
        "model = UNIPoint(FIXED_BATCH_SIZE, N_INPUT, N_NEURONS, N_PARAMETERS, N_BASIS_FUNCTIONS)\n",
        "print(model)\n",
        "output_val, states_val, params_value, basis_value, res = model(X_batch, TIME)\n",
        "print()\n",
        "print(output_val) # contains all output for all timesteps\n",
        "print()\n",
        "print(states_val) # contains values for final state or final timestep, i.e., t=1\n",
        "print()\n",
        "print(params_value) # contains values for parameters for final timesteps\n",
        "print()\n",
        "print(basis_value) # contains values of basis functions (now it is only ReLU functions)\n",
        "print()\n",
        "print(res) # contains values of SoftPlus activation"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIPoint(\n",
            "  (rnn): RNNCell(3, 1)\n",
            "  (h2p): Linear(in_features=1, out_features=2, bias=True)\n",
            "  (Softplus): Softplus(beta=1, threshold=20)\n",
            ")\n",
            "\n",
            "[tensor([[-0.0312],\n",
            "        [-0.3555],\n",
            "        [-0.9933],\n",
            "        [-1.0000]], grad_fn=<TanhBackward>), tensor([[-0.9760],\n",
            "        [-0.2468],\n",
            "        [-0.9711],\n",
            "        [-0.8572]], grad_fn=<TanhBackward>)]\n",
            "\n",
            "tensor([[-0.9760],\n",
            "        [-0.2468],\n",
            "        [-0.9711],\n",
            "        [-0.8572]], grad_fn=<TanhBackward>)\n",
            "\n",
            "tensor([[-0.8542,  0.1068],\n",
            "        [-0.1707,  0.0128],\n",
            "        [-0.8496,  0.1062],\n",
            "        [-0.7429,  0.0915]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "tensor([0., 0., 0., 0.])\n",
            "\n",
            "tensor(0.6931)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XoLGbXjjN4N"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
