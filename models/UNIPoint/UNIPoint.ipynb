{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIPoint",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Yfy2B76Z9Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgRk_PwDD4ct"
      },
      "source": [
        "# UNIPoint model\n",
        "class UNIPoint(nn.Module):\n",
        "    def __init__(self, batch_size, n_inputs, n_neurons, n_parameters, n_basis_functions):\n",
        "      \"\"\"\n",
        "      Input parameters:\n",
        "      n_neurons - number of neurons inside RNN\n",
        "      n_parameters - expecteed number of parameters in basis function\n",
        "      n_basis_functions - number of basis functions\n",
        "      \"\"\"\n",
        "      super(UNIPoint, self).__init__()\n",
        "        \n",
        "      self.rnn = nn.RNNCell(n_inputs, n_neurons)\n",
        "      self.hx = torch.randn(batch_size, n_neurons) # initialize hidden state \n",
        "      self.h2p = nn.Linear(n_neurons, n_parameters)\n",
        "      self.parameters = torch.randn(batch_size, n_parameters) # initialize parameters for intensity functions\n",
        "      self.basis_res1 = torch.randn(batch_size, n_parameters)\n",
        "      self.basis_res2 = torch.randn(batch_size, n_parameters)\n",
        "      self.Softplus = torch.nn.Softplus(beta = 1)\n",
        "      self.result = torch.randn(1)\n",
        "\n",
        "    def ReLU(self, parameters, time):\n",
        "      \"\"\"Function to apply Rectified Linear Unit (ReLU) as basis function inside network \n",
        "        Input parameters:\n",
        "          parameters - alpha, beta for basis function's value calculation\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "        \n",
        "      self.output = torch.relu(self.parameters[:,0] @ time + self.parameters[:,1]) \n",
        "      return self.output\n",
        "    \n",
        "    def PowerLaw(self, parameters, time):\n",
        "      \"\"\"Function to apply Power Law (PL) as basis function inside network \n",
        "        Input parameters:\n",
        "          parameters - alpha, beta for basis function's value calculation\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "      self.output = self.parameters[:,0] * (1 + time) ** (- self.parameters[:,1])\n",
        "      return self.output\n",
        "\n",
        "\n",
        "    def forward(self, X, time):\n",
        "      \"\"\"Input parameters:\n",
        "          X - batch with data\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "        \n",
        "      output = []\n",
        "\n",
        "      # for each time step\n",
        "      for i in range(2):\n",
        "          self.hx = self.rnn(X[i], self.hx)\n",
        "          self.parameters = self.h2p(self.hx)\n",
        "          self.basis_res1 = self.ReLU(self.parameters, time)\n",
        "          self.basis_res2 = self.PowerLaw(self.parameters, time)\n",
        "          self.result = self.Softplus(self.basis_res1 + self.basis_res2)\n",
        "\n",
        "          output.append(self.hx)\n",
        "\n",
        "        \n",
        "      return output, self.hx, self.parameters, self.basis_res1, self.basis_res2, self.result"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4NRCEyD4a4",
        "outputId": "0ee76deb-21cf-4fc6-ae60-26d850f78942"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "FIXED_BATCH_SIZE = 4 # our batch size is fixed for now\n",
        "N_INPUT = 3\n",
        "N_NEURONS = 1\n",
        "\n",
        "N_PARAMETERS = 2\n",
        "N_BASIS_FUNCTIONS = 8\n",
        "\n",
        "# for now we generate random values for time in [0,T]\n",
        "TIME = torch.from_numpy(np.sort(np.random.sample(FIXED_BATCH_SIZE))).type(torch.float) # TIME size is (FIXED_BATCH_SIZE, 1)\n",
        "\n",
        "X_batch = torch.tensor([[[0,1,2], [3,4,5], \n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        [[9,8,7], [0,0,0], \n",
        "                         [6,5,4], [3,2,1]]], dtype = torch.float) # X0 and X1\n",
        "\n",
        "\n",
        "model = UNIPoint(FIXED_BATCH_SIZE, N_INPUT, N_NEURONS, N_PARAMETERS, N_BASIS_FUNCTIONS)\n",
        "print(model)\n",
        "output_val, states_val, params_value, basis_value1, basis_value2, res = model(X_batch, TIME)\n",
        "print()\n",
        "print('output_val')\n",
        "print(output_val) # contains all output for all timesteps\n",
        "print()\n",
        "print('states_val')\n",
        "print(states_val) # contains values for final state or final timestep, i.e., t=1\n",
        "print()\n",
        "print('params_value')\n",
        "print(params_value) # contains values for parameters for final timesteps\n",
        "print()\n",
        "print('basis_value1')\n",
        "print(basis_value1) # contains values of basis functions (now it is ReLU)\n",
        "print()\n",
        "print('basis_value2')\n",
        "print(basis_value2) # contains values of basis functions (now it is Power Law)\n",
        "print()\n",
        "print('res')\n",
        "print(res) # contains values of SoftPlus activation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIPoint(\n",
            "  (rnn): RNNCell(3, 1)\n",
            "  (h2p): Linear(in_features=1, out_features=2, bias=True)\n",
            "  (Softplus): Softplus(beta=1, threshold=20)\n",
            ")\n",
            "\n",
            "output_val\n",
            "[tensor([[-0.8374],\n",
            "        [-0.9988],\n",
            "        [-1.0000],\n",
            "        [ 0.8818]], grad_fn=<TanhBackward>), tensor([[-1.0000],\n",
            "        [-0.0606],\n",
            "        [-0.9990],\n",
            "        [-0.8425]], grad_fn=<TanhBackward>)]\n",
            "\n",
            "states_val\n",
            "tensor([[-1.0000],\n",
            "        [-0.0606],\n",
            "        [-0.9990],\n",
            "        [-0.8425]], grad_fn=<TanhBackward>)\n",
            "\n",
            "params_value\n",
            "tensor([[0.8359, 1.0991],\n",
            "        [0.8950, 0.1929],\n",
            "        [0.8360, 1.0982],\n",
            "        [0.8458, 0.9471]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "basis_value1\n",
            "tensor([2.7156, 1.8094, 2.7147, 2.5637], grad_fn=<ReluBackward0>)\n",
            "\n",
            "basis_value2\n",
            "tensor([0.6745, 0.8342, 0.4953, 0.5329], grad_fn=<MulBackward0>)\n",
            "\n",
            "res\n",
            "tensor([3.4233, 2.7123, 3.2496, 3.1408], grad_fn=<SoftplusBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1s0915AnUEM"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}
