{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIPoint",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Yfy2B76Z9Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgRk_PwDD4ct"
      },
      "source": [
        "# UNIPoint model\n",
        "#-------------------------------------------------------\n",
        "# dimensions be like:\n",
        "#-------------------------------------------------------\n",
        "# input -> hidden state -> parameters -> basis functions -> intensity function (output)\n",
        "#-------------------------------------------------------\n",
        "# (batch_size, seq_len, n_features) -> [for iteration we take input] (batch_size, 1, n_features) ->\n",
        "# -> (batch_size, 1, 1) -> (batch_size, 1, n_parameters * n_basis_functions) -> \n",
        "# -> (batch_size, 1, n_basis_functions) -> (batch_size, 1, 1) \n",
        "#-------------------------------------------------------\n",
        "\n",
        "\n",
        "class UNIPoint(nn.Module):\n",
        "    def __init__(self, batch_size, seq_len, n_features, n_parameters, n_basis_functions):\n",
        "      \"\"\"\n",
        "      Input parameters:\n",
        "      n_neurons - number of neurons inside RNN\n",
        "      n_parameters - expecteed number of parameters in basis function\n",
        "      n_basis_functions - number of basis functions\n",
        "      \"\"\"\n",
        "      super(UNIPoint, self).__init__()\n",
        "\n",
        "      self.rnn = nn.RNNCell(n_features, 1)\n",
        "      self.hx = torch.randn(batch_size, 1) # initialize hidden state \n",
        "      self.h2p = nn.Linear(1, n_parameters * n_basis_functions)\n",
        "      self.basis_res = torch.randn(batch_size, n_basis_functions) #initialize matrix for basis f-s calculations results\n",
        "      self.Softplus = torch.nn.Softplus(beta = 1)\n",
        "      self.n_basis_functions = n_basis_functions\n",
        "\n",
        "    def ReLU(self, parameter_1, parameter_2, time):\n",
        "      \"\"\"Function to apply Rectified Linear Unit (ReLU) as basis function inside network \n",
        "        Input parameters:\n",
        "          parameters - alpha, beta for basis function's value calculation\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "      #print(parameter_1, 'is an ordinal number of parameter_1 and self.parameters[:,parameter_1] is ', self.parameters[:,parameter_1])\n",
        "      #print(parameter_2, 'is an ordinal number of parameter_2 and self.parameters[:,parameter_2] is ', self.parameters[:,parameter_2])\n",
        "      #print()\n",
        "      self.output = torch.relu(self.parameters[:,parameter_1] * time + self.parameters[:,parameter_2] ) \n",
        "      return self.output\n",
        "    \n",
        "    def PowerLaw(self, parameters, time): # need to fix (see ReLU parameters and do the same)\n",
        "      \"\"\"Function to apply Power Law (PL) as basis function inside network \n",
        "        Input parameters:\n",
        "          parameters - alpha, beta for basis function's value calculation\n",
        "          time - column-vector with time which had been spent since the begining of \n",
        "                  temporal point process (TPP)\n",
        "      \"\"\"\n",
        "      self.output = self.parameters[0] * (1 + time)**( - self.parameters[1])\n",
        "      return self.output\n",
        "\n",
        "\n",
        "    def forward(self, X, time):\n",
        "      \"\"\"Input parameters:\n",
        "          X - batch with data \n",
        "          time - column-vector with interarrival time in temporal point process (TPP)\n",
        "      \"\"\"\n",
        "        \n",
        "      output = []\n",
        "\n",
        "      print(\"------------Learning process starts here------------\")\n",
        "      print()\n",
        "      # for each time step (here X shape is (batch_size, seq_len, n_features) )\n",
        "      for i in range(X.shape[1]):\n",
        "\n",
        "          #print(X[:,i,:].shape, 'is X shape')\n",
        "          #print(self.hx.shape, 'is self.hx shape')\n",
        "          self.hx = self.rnn(X[:,i,:], self.hx)\n",
        "          self.parameters = self.h2p(self.hx)\n",
        "          \n",
        "          for function in range(self.n_basis_functions): \n",
        "              # calculating numbers of parameters to take for basis function\n",
        "              par1 = 2 * function\n",
        "              par2 = 2 * function + 1\n",
        "              \n",
        "              #print(\"'function' iterator value is \", function)\n",
        "              #print('par1, par2 are ', par1, par2)\n",
        "              #print('X[:,i,:] shape is ', X[:,i,:].shape)\n",
        "              #print()\n",
        "              #print(self.basis_res[:, function].shape, 'is a shape of self.basis_res[:, function]')\n",
        "              self.basis_res[:, function] = self.ReLU(par1, par2, X[:,i,1]) # here X[:,i,1] - tau\n",
        "          \n",
        "          self.sum_res = torch.sum(self.basis_res, 1)\n",
        "\n",
        "          self.result = self.Softplus(self.sum_res)\n",
        "\n",
        "          output.append(self.hx)\n",
        "          \n",
        "          print(\"Epoch\", i+1, \"out of\", X.shape[1])\n",
        "          print()\n",
        "\n",
        "      print(\"------------Learning process is finished------------\")\n",
        "\n",
        "      return output, self.hx, self.parameters, self.basis_res, self.sum_res, self.result\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar4NRCEyD4a4",
        "outputId": "f523d934-6f07-40a9-d7ad-04d4ce0ca7cd"
      },
      "source": [
        "# model evaluation\n",
        "\n",
        "# X_batch dimension = (batch_size, seq_len, n_features)\n",
        "X_batch = torch.tensor([[[1, 0.1],\n",
        "                         [2, 0.2],\n",
        "                         [3, 0.3],\n",
        "                         [4, 0.4],\n",
        "                         [5, 0.5],\n",
        "                         [6, 0.6],\n",
        "                         [7, 0.7]],\n",
        "                        [[10, 1.0],\n",
        "                         [8, 0.8], \n",
        "                         [6, 0.6],\n",
        "                         [4, 0.4],\n",
        "                         [2, 0.2],\n",
        "                         [1, 0.1],\n",
        "                         [2, 0.2]]], dtype = torch.float)\n",
        "\n",
        "#print(\"shape of X_batch: \", X_batch.shape)\n",
        "\n",
        "TIME = X_batch[:,:,1].detach().clone()\n",
        "#for i in range(7):\n",
        "#  print(X_batch[:,i,:])\n",
        "#  print('this sequence shape is ', X_batch[:,i,:].shape)\n",
        "#  print()\n",
        "\n",
        "FIXED_BATCH_SIZE = 2 # our batch size is fixed for now\n",
        "SEQ_LEN = 7\n",
        "N_FEATURES = 2\n",
        "#N_NEURONS = 1\n",
        "\n",
        "N_PARAMETERS = 2\n",
        "N_BASIS_FUNCTIONS = 4\n",
        "\n",
        "\n",
        "model = UNIPoint(FIXED_BATCH_SIZE, SEQ_LEN, N_FEATURES, N_PARAMETERS, N_BASIS_FUNCTIONS)\n",
        "print(model)\n",
        "output_val, hid_states_val, params_value, basis_results, sum_results, res = model(X_batch, TIME)\n",
        "print()\n",
        "print('output_val')\n",
        "print(output_val) # contains all output for all timesteps\n",
        "print()\n",
        "print('hid_states_val')\n",
        "print(hid_states_val) # contains values for final state or final timestep, i.e., t=1\n",
        "print()\n",
        "print('params_value')\n",
        "print(params_value) # contains values for parameters for final timesteps\n",
        "print()\n",
        "print('basis_results')\n",
        "print(basis_results) # contains values of basis functions (now it is ReLU)\n",
        "print()\n",
        "print('sum_results')\n",
        "print(sum_results) # contains values of basis functions (now it is Power Law)\n",
        "print()\n",
        "print('res')\n",
        "print(res) # contains values of SoftPlus activation"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIPoint(\n",
            "  (rnn): RNNCell(2, 1)\n",
            "  (h2p): Linear(in_features=1, out_features=8, bias=True)\n",
            "  (Softplus): Softplus(beta=1, threshold=20)\n",
            ")\n",
            "------------Learning process starts here------------\n",
            "\n",
            "Epoch 1 out of 7\n",
            "\n",
            "Epoch 2 out of 7\n",
            "\n",
            "Epoch 3 out of 7\n",
            "\n",
            "Epoch 4 out of 7\n",
            "\n",
            "Epoch 5 out of 7\n",
            "\n",
            "Epoch 6 out of 7\n",
            "\n",
            "Epoch 7 out of 7\n",
            "\n",
            "------------Learning process is finished------------\n",
            "\n",
            "output_val\n",
            "[tensor([[-0.5138],\n",
            "        [-1.0000]], grad_fn=<TanhBackward>), tensor([[-0.8505],\n",
            "        [-0.9999]], grad_fn=<TanhBackward>), tensor([[-0.9491],\n",
            "        [-0.9990]], grad_fn=<TanhBackward>), tensor([[-0.9855],\n",
            "        [-0.9850]], grad_fn=<TanhBackward>), tensor([[-0.9961],\n",
            "        [-0.8057]], grad_fn=<TanhBackward>), tensor([[-0.9990],\n",
            "        [-0.4640]], grad_fn=<TanhBackward>), tensor([[-0.9997],\n",
            "        [-0.8546]], grad_fn=<TanhBackward>)]\n",
            "\n",
            "hid_states_val\n",
            "tensor([[-0.9997],\n",
            "        [-0.8546]], grad_fn=<TanhBackward>)\n",
            "\n",
            "params_value\n",
            "tensor([[-0.3036,  0.4454, -0.1162, -0.5759,  0.1225, -1.2125, -0.5560,  1.2701],\n",
            "        [-0.3532,  0.3982,  0.0247, -0.4406,  0.2010, -1.1811, -0.4238,  1.2278]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "\n",
            "basis_results\n",
            "tensor([[0.2329, 0.0000, 0.0000, 0.8809],\n",
            "        [0.3275, 0.0000, 0.0000, 1.1431]], grad_fn=<CopySlices>)\n",
            "\n",
            "sum_results\n",
            "tensor([1.1137, 1.4706], grad_fn=<SumBackward1>)\n",
            "\n",
            "res\n",
            "tensor([1.3977, 1.6774], grad_fn=<SoftplusBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMN0Rwspwc_p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
