{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIPoint_Stepanov.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw2croG95_XE"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch import optim\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from time import time\r\n",
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_BuwUCOWYry",
        "outputId": "0ded41cc-d639-46fb-e47b-80766b147e55"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH09CmZk4bc0",
        "outputId": "3f895f21-15c0-4941-ec4c-77422924d9ef"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 13 12:47:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    11W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R51TRvHx6jvL",
        "outputId": "07cd0771-1e10-4fef-c240-03d03f682427"
      },
      "source": [
        "!git clone https://username:password@github.com/rodrigorivera/mds20_deepfolio\r\n",
        "from mds20_deepfolio.models.NeuralHawkesProcess.DataWrapper import NHPDataset\r\n",
        "\r\n",
        "!unzip /content/mds20_deepfolio/models/NeuralHawkesProcess/data/fin_data.zip \\\r\n",
        "      -d /content/mds20_deepfolio/models/NeuralHawkesProcess/data/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mds20_deepfolio'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 759 (delta 13), reused 0 (delta 0), pack-reused 726\u001b[K\n",
            "Receiving objects: 100% (759/759), 70.05 MiB | 38.21 MiB/s, done.\n",
            "Resolving deltas: 100% (435/435), done.\n",
            "Archive:  /content/mds20_deepfolio/models/NeuralHawkesProcess/data/fin_data.zip\n",
            "  inflating: /content/mds20_deepfolio/models/NeuralHawkesProcess/data/test.pkl  \n",
            "  inflating: /content/mds20_deepfolio/models/NeuralHawkesProcess/data/dev.pkl  \n",
            "  inflating: /content/mds20_deepfolio/models/NeuralHawkesProcess/data/train.pkl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xUcPm3ODmv7"
      },
      "source": [
        "## NHP article data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUbht85K9g5z"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "class NHPDataset(Dataset):\r\n",
        "    ''' \r\n",
        "    Create Dataset for Neural Hawkey Process\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, file_path):\r\n",
        "        self.event_type = []\r\n",
        "        self.event_time = []\r\n",
        "\r\n",
        "        with open(file_path, 'rb') as f:\r\n",
        "\r\n",
        "            if 'dev' in file_path:\r\n",
        "                seqs = pickle.load(f, encoding='latin1')['dev']\r\n",
        "            elif 'train' in file_path:\r\n",
        "                seqs = pickle.load(f, encoding='latin1')['train']\r\n",
        "            elif 'test' in file_path:\r\n",
        "                seqs = pickle.load(f, encoding='latin1')['test']\r\n",
        "\r\n",
        "            for idx, seq in enumerate(seqs):\r\n",
        "                self.event_type.append(torch.Tensor([int(event['type_event']) for event in seq]))\r\n",
        "                self.event_time.append(torch.Tensor([float(event['time_since_start']) for event in seq]))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.event_type)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "\r\n",
        "        event_type = torch.LongTensor(self.event_type[index].long())\r\n",
        "        event_time = torch.Tensor(self.event_time[index])\r\n",
        "        delta_time = torch.zeros_like(event_time)\r\n",
        "        delta_time[1:] = event_time[1:] - event_time[:-1]\r\n",
        "        \r\n",
        "        return delta_time, event_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-GXNf7K_rVg"
      },
      "source": [
        "train_dataset = NHPDataset('/content/mds20_deepfolio/models/NeuralHawkesProcess/data/train.pkl')\r\n",
        "val_dataset = NHPDataset('/content/mds20_deepfolio/models/NeuralHawkesProcess/data/dev.pkl')\r\n",
        "test_dataset = NHPDataset('/content/mds20_deepfolio/models/NeuralHawkesProcess/data/test.pkl')\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=12)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=12)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBFHyoj1zB8M",
        "outputId": "0a5f85e8-8c55-4699-9c99-2ded0beb990a"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0000, 0.1000, 2.1500,  ..., 0.3833, 1.3000, 0.0166]),\n",
              " tensor([1, 1, 1,  ..., 1, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnOYtmAKIhxc"
      },
      "source": [
        "## LTC preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIOHuCDaI8-j",
        "outputId": "67aff1aa-3d4d-4afc-e9d0-1debc6411168"
      },
      "source": [
        "dataset_LTC = np.load(\"/content/drive/MyDrive/unipoint/dataset_LTC.npy\", allow_pickle = True)\r\n",
        "dataset_LTC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60470000e+04, 3.60470000e+04, 0.00000000e+00],\n",
              "       [4.18310000e+04, 7.78780000e+04, 0.00000000e+00],\n",
              "       [1.51073000e+05, 2.28951000e+05, 0.00000000e+00],\n",
              "       ...,\n",
              "       [4.12810000e+04, 3.15356062e+10, 0.00000000e+00],\n",
              "       [2.04746000e+05, 3.15358109e+10, 0.00000000e+00],\n",
              "       [1.71130000e+04, 3.15358280e+10, 1.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSwU7s6IJdtK",
        "outputId": "45433ebe-d4a1-44d0-88c9-0ae82a489a68"
      },
      "source": [
        "# drop 'time from 0' column\r\n",
        "dataset_LTC = np.delete(dataset_LTC, 1, axis = 1)\r\n",
        "dataset_LTC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60470e+04, 0.00000e+00],\n",
              "       [4.18310e+04, 0.00000e+00],\n",
              "       [1.51073e+05, 0.00000e+00],\n",
              "       ...,\n",
              "       [4.12810e+04, 0.00000e+00],\n",
              "       [2.04746e+05, 0.00000e+00],\n",
              "       [1.71130e+04, 1.00000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWDh_6-IJdql",
        "outputId": "d057a0f6-0d4f-41b5-ac39-cc267b5b8597"
      },
      "source": [
        "dataset_LTC = np.transpose(dataset_LTC)\r\n",
        "dataset_LTC"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60470e+04, 4.18310e+04, 1.51073e+05, ..., 4.12810e+04,\n",
              "        2.04746e+05, 1.71130e+04],\n",
              "       [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00,\n",
              "        0.00000e+00, 1.00000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1yjGKTxM93W",
        "outputId": "eaff8db1-faaa-4436-c6a0-fb778a6ac9d2"
      },
      "source": [
        "dataset_LTC.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "901989"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO0Qsa5IJdn2",
        "outputId": "6aaf80d5-d465-49a2-9084-b1a15bee4e63"
      },
      "source": [
        "time_LTC_zeros = []\r\n",
        "time_LTC_ones = []\r\n",
        "\r\n",
        "# for converting miliseconds to seconds in the first raws it is needed to divide values by 1000\r\n",
        "\r\n",
        "for i in range(dataset_LTC.shape[1]):\r\n",
        "  if dataset_LTC[1,i] == 0:\r\n",
        "    time_LTC_zeros.append(dataset_LTC[0,i]/1000)\r\n",
        "  elif dataset_LTC[1,i] == 1:\r\n",
        "    time_LTC_ones.append(dataset_LTC[0,i]/1000)\r\n",
        "\r\n",
        "line_zeros = np.zeros_like(time_LTC_zeros)\r\n",
        "line_ones = np.ones_like(time_LTC_ones)\r\n",
        "\r\n",
        "dataset_zeros = np.array([time_LTC_zeros, line_zeros])\r\n",
        "dataset_ones = np.array([time_LTC_ones, line_ones])\r\n",
        "\r\n",
        "print(dataset_zeros)\r\n",
        "print()\r\n",
        "print(dataset_ones)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 36.047  41.831 151.073 ...  14.893  41.281 204.746]\n",
            " [  0.      0.      0.    ...   0.      0.      0.   ]]\n",
            "\n",
            "[[ 98.221  57.206  38.969 ...  52.958 108.255  17.113]\n",
            " [  1.      1.      1.    ...   1.      1.      1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LQ4hWhpI9CZ",
        "outputId": "6d1d1aad-993b-4bda-c457-1341ad906720"
      },
      "source": [
        "dataset_zeros.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 461927)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj3WqFztSP3f",
        "outputId": "a19b0aee-8dee-4b4f-d495-9110a0dfea7b"
      },
      "source": [
        "dataset_ones.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 440062)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdmEOIzWG3zt"
      },
      "source": [
        "# now we need to divide prepared data into train:val:test = 60:20:20\r\n",
        "proportion_zeros = [int(0.6 * dataset_zeros.shape[1]), int(0.8 * dataset_zeros.shape[1])]\r\n",
        "proportion_ones = [int(0.6 * dataset_ones.shape[1]), int(0.8 * dataset_ones.shape[1])]\r\n",
        "\r\n",
        "# cut zeros dataset into the train, val and test\r\n",
        "dataset_zeros_split = np.hsplit(dataset_zeros, proportion_zeros)\r\n",
        "dataset_ones_split = np.hsplit(dataset_ones, proportion_ones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsmhvdMLHyTc",
        "outputId": "a82cb9ba-0e82-4b38-b796-5fc8a886a3b2"
      },
      "source": [
        "num_seq = 3319 # num of sequences in every future batch\r\n",
        "\r\n",
        "# FOR ZEROS\r\n",
        "for part_idx, part in enumerate(dataset_zeros_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_zeros = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_zeros_cut = np.hsplit(part, cut_zeros)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_zeros_cut.pop(0)\r\n",
        "  dataset_zeros_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_zeros_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/train_LTC/train_LTC_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/val_LTC/val_LTC_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/test_LTC/test_LTC_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "\r\n",
        "# FOR ONES\r\n",
        "for part_idx, part in enumerate(dataset_ones_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_ones = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_ones_cut = np.hsplit(part, cut_ones)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_ones_cut.pop(0)\r\n",
        "  dataset_ones_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_ones_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/train_LTC/train_LTC_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/val_LTC/val_LTC_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_LTC/test_LTC/test_LTC_ones_' + str(cut_idx) + '.npy', cut)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [[ 36.047  41.831 151.073 ...   9.797  10.335  38.561]\n",
            " [  0.      0.      0.    ...   0.      0.      0.   ]]\n",
            "1 [[ 3.959  9.322 27.339 ... 59.167 32.848 12.369]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "2 [[  3.142  10.755   9.834 ...  14.893  41.281 204.746]\n",
            " [  0.      0.      0.    ...   0.      0.      0.   ]]\n",
            "0 [[98.221 57.206 38.969 ... 21.887 34.529 12.957]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "1 [[ 24.981   9.455  10.907 ... 129.162 175.393  71.247]\n",
            " [  1.      1.      1.    ...   1.      1.      1.   ]]\n",
            "2 [[101.371 129.248  88.038 ...  52.958 108.255  17.113]\n",
            " [  1.      1.      1.    ...   1.      1.      1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAaE55HP9IlR"
      },
      "source": [
        "import glob\r\n",
        "\r\n",
        "class TupoDataset(Dataset):\r\n",
        "    def __init__(self, data_folder, data_type = '.npy'):\r\n",
        "\r\n",
        "        self.file_paths = glob.glob(data_folder+'*'+data_type)\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "\r\n",
        "        return len(self.file_paths)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "\r\n",
        "        sample = np.load(self.file_paths[index])\r\n",
        "        event_time = torch.Tensor(sample[0,:])\r\n",
        "        event_type = torch.LongTensor(sample[1,:])\r\n",
        "        \r\n",
        "        return event_time, event_type"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYXE-Tm7I9o"
      },
      "source": [
        "train_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_LTC/train_LTC/')\r\n",
        "val_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_LTC/val_LTC/')\r\n",
        "test_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_LTC/test_LTC/')\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=12)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=12)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=12)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPCstqxt7I7B",
        "outputId": "fc85d80c-00a4-41c7-ff55-6c1bb16df8f7"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 7.9540, 20.2040, 10.3260,  ..., 90.1850, 20.2220, 14.3900]),\n",
              " tensor([0, 0, 0,  ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aA9aOBeIoJf"
      },
      "source": [
        "## EOS preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AHpR427I0vm",
        "outputId": "e215eef9-a4a7-4898-ef18-1d3c342e1d93"
      },
      "source": [
        "dataset_EOS = np.load(\"/content/drive/MyDrive/unipoint/dataset_EOS.npy\", allow_pickle = True)\r\n",
        "\r\n",
        "\r\n",
        "# drop 'time from 0' column\r\n",
        "dataset_EOS = np.delete(dataset_EOS, 1, axis = 1)\r\n",
        "dataset_EOS = np.transpose(dataset_EOS)\r\n",
        "\r\n",
        "time_EOS_zeros = []\r\n",
        "time_EOS_ones = []\r\n",
        "\r\n",
        "# for converting miliseconds to seconds in the first raws it is needed to divide values by 1000\r\n",
        "\r\n",
        "for i in range(dataset_EOS.shape[1]):\r\n",
        "  if dataset_EOS[1,i] == 0:\r\n",
        "    time_EOS_zeros.append(dataset_EOS[0,i]/1000)\r\n",
        "  elif dataset_EOS[1,i] == 1:\r\n",
        "    time_EOS_ones.append(dataset_EOS[0,i]/1000)\r\n",
        "\r\n",
        "line_zeros = np.zeros_like(time_EOS_zeros)\r\n",
        "line_ones = np.ones_like(time_EOS_ones)\r\n",
        "\r\n",
        "dataset_zeros = np.array([time_EOS_zeros, line_zeros])\r\n",
        "dataset_ones = np.array([time_EOS_ones, line_ones])\r\n",
        "\r\n",
        "print(dataset_zeros)\r\n",
        "print()\r\n",
        "print(dataset_ones)\r\n",
        "\r\n",
        "dataset_zeros.shape\r\n",
        "dataset_ones.shape\r\n",
        "\r\n",
        "# now we need to divide prepared data into train:val:test = 60:20:20\r\n",
        "proportion_zeros = [int(0.6 * dataset_zeros.shape[1]), int(0.8 * dataset_zeros.shape[1])]\r\n",
        "proportion_ones = [int(0.6 * dataset_ones.shape[1]), int(0.8 * dataset_ones.shape[1])]\r\n",
        "\r\n",
        "# cut zeros dataset into the train, val and test\r\n",
        "dataset_zeros_split = np.hsplit(dataset_zeros, proportion_zeros)\r\n",
        "dataset_ones_split = np.hsplit(dataset_ones, proportion_ones)\r\n",
        "num_seq = 3319 # num of sequences in every future batch\r\n",
        "\r\n",
        "# FOR ZEROS\r\n",
        "for part_idx, part in enumerate(dataset_zeros_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_zeros = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_zeros_cut = np.hsplit(part, cut_zeros)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_zeros_cut.pop(0)\r\n",
        "  dataset_zeros_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_zeros_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/train_EOS/train_EOS_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/val_EOS/val_EOS_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/test_EOS/test_EOS_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "\r\n",
        "# FOR ONES\r\n",
        "for part_idx, part in enumerate(dataset_ones_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_ones = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_ones_cut = np.hsplit(part, cut_ones)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_ones_cut.pop(0)\r\n",
        "  dataset_ones_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_ones_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/train_EOS/train_EOS_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/val_EOS/val_EOS_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_EOS/test_EOS/test_EOS_ones_' + str(cut_idx) + '.npy', cut)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  56.914   11.428   29.426 ...  516.549  177.156 1362.134]\n",
            " [   0.       0.       0.    ...    0.       0.       0.   ]]\n",
            "\n",
            "[[ 17.598  14.441  96.329 ... 488.478 128.664 990.835]\n",
            " [  1.      1.      1.    ...   1.      1.      1.   ]]\n",
            "0 [[56.914 11.428 29.426 ... 17.61  18.524 12.906]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "1 [[  6.752  11.854  12.483 ... 110.148  60.012  50.523]\n",
            " [  0.      0.      0.    ...   0.      0.      0.   ]]\n",
            "2 [[  95.326   73.795   74.946 ...  516.549  177.156 1362.134]\n",
            " [   0.       0.       0.    ...    0.       0.       0.   ]]\n",
            "0 [[17.598 14.441 96.329 ...  9.933  9.367 30.375]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "1 [[16.38  25.336 11.403 ...  2.116  3.948  6.946]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "2 [[ 13.006  25.64   10.156 ... 488.478 128.664 990.835]\n",
            " [  1.      1.      1.    ...   1.      1.      1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3f3A6-kIqe_"
      },
      "source": [
        "train_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_EOS/train_EOS/')\r\n",
        "val_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_EOS/val_EOS/')\r\n",
        "test_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_EOS/test_EOS/')\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=12)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=12)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3iBNbk3Iqbo",
        "outputId": "1cc53ab6-b7b5-4aed-c769-c2cf3bdab2a7"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([53.4870, 38.2990, 20.5250,  ..., 19.7070, 50.5350, 79.8570]),\n",
              " tensor([0, 0, 0,  ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GteTy9wrRnt_"
      },
      "source": [
        "## Data with bids preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0oxSDxkRovk",
        "outputId": "a09ee13b-df2e-4ced-fb3e-2723e7e13921"
      },
      "source": [
        "dataset_transact = np.load(\"/content/drive/MyDrive/unipoint/transact_dataset_ETH.npy\", allow_pickle = True)\r\n",
        "dataset_transact"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[          0,           0,           1],\n",
              "       [        477,         477,           0],\n",
              "       [       1176,         699,           1],\n",
              "       ...,\n",
              "       [31535986960,        8189,           1],\n",
              "       [31535992147,        5187,           1],\n",
              "       [31535995183,        3036,           1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbh0GYHXRpxL",
        "outputId": "7dd65c6d-8749-4f37-de99-4c4494226afe"
      },
      "source": [
        "# drop 'time from 0' column\r\n",
        "dataset_transact = np.delete(dataset_transact, 0, axis = 1)\r\n",
        "dataset_transact = np.transpose(dataset_transact)\r\n",
        "\r\n",
        "time_transact_zeros = []\r\n",
        "time_transact_ones = []\r\n",
        "\r\n",
        "# for converting miliseconds to seconds in the first raws it is needed to divide values by 1000\r\n",
        "\r\n",
        "for i in range(dataset_transact.shape[1]):\r\n",
        "  if dataset_transact[1,i] == 0:\r\n",
        "    time_transact_zeros.append(dataset_transact[0,i]/1000)\r\n",
        "  elif dataset_transact[1,i] == 1:\r\n",
        "    time_transact_ones.append(dataset_transact[0,i]/1000)\r\n",
        "\r\n",
        "line_zeros = np.zeros_like(time_transact_zeros)\r\n",
        "line_ones = np.ones_like(time_transact_ones)\r\n",
        "\r\n",
        "dataset_zeros = np.array([time_transact_zeros, line_zeros])\r\n",
        "dataset_ones = np.array([time_transact_ones, line_ones])\r\n",
        "\r\n",
        "print(dataset_zeros)\r\n",
        "print()\r\n",
        "print(dataset_ones)\r\n",
        "\r\n",
        "dataset_zeros.shape\r\n",
        "dataset_ones.shape\r\n",
        "\r\n",
        "# now we need to divide prepared data into train:val:test = 60:20:20\r\n",
        "proportion_zeros = [int(0.6 * dataset_zeros.shape[1]), int(0.8 * dataset_zeros.shape[1])]\r\n",
        "proportion_ones = [int(0.6 * dataset_ones.shape[1]), int(0.8 * dataset_ones.shape[1])]\r\n",
        "\r\n",
        "# cut zeros dataset into the train, val and test\r\n",
        "dataset_zeros_split = np.hsplit(dataset_zeros, proportion_zeros)\r\n",
        "dataset_ones_split = np.hsplit(dataset_ones, proportion_ones)\r\n",
        "num_seq = 3319 # num of sequences in every future batch\r\n",
        "\r\n",
        "# FOR ZEROS\r\n",
        "for part_idx, part in enumerate(dataset_zeros_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_zeros = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_zeros_cut = np.hsplit(part, cut_zeros)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_zeros_cut.pop(0)\r\n",
        "  dataset_zeros_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_zeros_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/train_transact/train_transact_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/val_transact/val_transact_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/test_transact/test_transact_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "\r\n",
        "# FOR ONES\r\n",
        "for part_idx, part in enumerate(dataset_ones_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_ones = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_ones_cut = np.hsplit(part, cut_ones)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_ones_cut.pop(0)\r\n",
        "  dataset_ones_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_ones_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/train_transact/train_transact_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/val_transact/val_transact_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_transact/test_transact/test_transact_ones_' + str(cut_idx) + '.npy', cut)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.477 0.68  2.179 ... 2.119 4.656 8.042]\n",
            " [0.    0.    0.    ... 0.    0.    0.   ]]\n",
            "\n",
            "[[0.    0.699 0.818 ... 8.189 5.187 3.036]\n",
            " [1.    1.    1.    ... 1.    1.    1.   ]]\n",
            "0 [[0.477 0.68  2.179 ... 0.112 0.817 0.   ]\n",
            " [0.    0.    0.    ... 0.    0.    0.   ]]\n",
            "1 [[0.635 0.144 0.08  ... 0.276 0.204 0.   ]\n",
            " [0.    0.    0.    ... 0.    0.    0.   ]]\n",
            "2 [[0.    1.193 1.02  ... 2.119 4.656 8.042]\n",
            " [0.    0.    0.    ... 0.    0.    0.   ]]\n",
            "0 [[0.    0.699 0.818 ... 0.    0.433 0.137]\n",
            " [1.    1.    1.    ... 1.    1.    1.   ]]\n",
            "1 [[0.343 0.153 0.211 ... 0.311 0.212 1.005]\n",
            " [1.    1.    1.    ... 1.    1.    1.   ]]\n",
            "2 [[0.886 1.758 0.679 ... 8.189 5.187 3.036]\n",
            " [1.    1.    1.    ... 1.    1.    1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az9itXz0UY8Y"
      },
      "source": [
        "train_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_transact/train_transact/')\r\n",
        "val_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_transact/val_transact/')\r\n",
        "test_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_transact/test_transact/')\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=12)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=12)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIpRPFqORpt0",
        "outputId": "31dd3af7-4bac-4221-d481-bc81fc7ee15b"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([6.0000e-03, 1.0000e-03, 1.0600e-01,  ..., 0.0000e+00, 4.7600e-01,\n",
              "         2.2250e+00]), tensor([0, 0, 0,  ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO_5E1WWXC5E"
      },
      "source": [
        "## ETH preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgIq_YhFZTHR",
        "outputId": "8d78199a-6676-4884-93bc-23cb89a64baa"
      },
      "source": [
        "dataset_ETH = np.load(\"/content/drive/MyDrive/unipoint/dataset_filtered_ETH.npy\", allow_pickle = True)\r\n",
        "print(dataset_ETH.shape)\r\n",
        "dataset_ETH"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(527, 3000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.42720000e+04, 1.42720000e+04, 1.00000000e+00],\n",
              "        [1.33890000e+04, 2.76610000e+04, 0.00000000e+00],\n",
              "        [2.73870000e+04, 5.50480000e+04, 0.00000000e+00],\n",
              "        ...,\n",
              "        [1.13620000e+04, 7.58121140e+07, 0.00000000e+00],\n",
              "        [2.33570000e+04, 7.58354710e+07, 0.00000000e+00],\n",
              "        [7.34500000e+03, 7.58428160e+07, 1.00000000e+00]],\n",
              "\n",
              "       [[1.48670000e+04, 7.58576830e+07, 1.00000000e+00],\n",
              "        [2.38870000e+04, 7.58815700e+07, 1.00000000e+00],\n",
              "        [5.42900000e+03, 7.58869990e+07, 1.00000000e+00],\n",
              "        ...,\n",
              "        [1.23110000e+04, 1.21968785e+08, 0.00000000e+00],\n",
              "        [1.87530000e+04, 1.21987538e+08, 1.00000000e+00],\n",
              "        [1.44190000e+04, 1.22001957e+08, 1.00000000e+00]],\n",
              "\n",
              "       [[8.20800000e+03, 1.22010165e+08, 0.00000000e+00],\n",
              "        [4.40500000e+03, 1.22014570e+08, 0.00000000e+00],\n",
              "        [9.40300000e+03, 1.22023973e+08, 0.00000000e+00],\n",
              "        ...,\n",
              "        [5.06200000e+03, 1.49642586e+08, 1.00000000e+00],\n",
              "        [3.09500000e+03, 1.49645681e+08, 1.00000000e+00],\n",
              "        [1.17200000e+03, 1.49646853e+08, 0.00000000e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1.16940000e+04, 3.07805427e+10, 1.00000000e+00],\n",
              "        [9.24400000e+03, 3.07805520e+10, 1.00000000e+00],\n",
              "        [2.81360000e+04, 3.07805801e+10, 1.00000000e+00],\n",
              "        ...,\n",
              "        [2.88020000e+04, 3.08376021e+10, 1.00000000e+00],\n",
              "        [1.87200000e+04, 3.08376208e+10, 0.00000000e+00],\n",
              "        [8.58500000e+03, 3.08376294e+10, 0.00000000e+00]],\n",
              "\n",
              "       [[3.70570000e+04, 3.10480134e+10, 1.00000000e+00],\n",
              "        [2.58070000e+04, 3.10480392e+10, 1.00000000e+00],\n",
              "        [1.15820000e+04, 3.10480508e+10, 0.00000000e+00],\n",
              "        ...,\n",
              "        [2.55280000e+04, 3.11318489e+10, 0.00000000e+00],\n",
              "        [2.54960000e+04, 3.11318744e+10, 0.00000000e+00],\n",
              "        [2.64750000e+04, 3.11319009e+10, 1.00000000e+00]],\n",
              "\n",
              "       [[2.99440000e+04, 3.13323880e+10, 1.00000000e+00],\n",
              "        [4.13930000e+04, 3.13324294e+10, 0.00000000e+00],\n",
              "        [2.36870000e+04, 3.13324531e+10, 1.00000000e+00],\n",
              "        ...,\n",
              "        [5.29500000e+03, 3.13981226e+10, 0.00000000e+00],\n",
              "        [9.34700000e+03, 3.13981319e+10, 1.00000000e+00],\n",
              "        [8.67800000e+03, 3.13981406e+10, 1.00000000e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3sxU-FZz4T"
      },
      "source": [
        "# drop 'time from 0' column\r\n",
        "dataset_ETH = np.delete(dataset_ETH[:,:,], 1, axis = 2)\r\n",
        "dataset_ETH = dataset_ETH.reshape(-1,2)\r\n",
        "dataset_ETH = np.transpose(dataset_ETH)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HutTBEi5Z3E4",
        "outputId": "fc888d44-9960-4273-88ca-d3977deacadf"
      },
      "source": [
        "dataset_ETH"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4272e+04, 1.3389e+04, 2.7387e+04, ..., 5.2950e+03, 9.3470e+03,\n",
              "        8.6780e+03],\n",
              "       [1.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 1.0000e+00,\n",
              "        1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Es8NEdXHP1",
        "outputId": "737be0b3-b30b-4c44-bf76-043d417fbce8"
      },
      "source": [
        "time_ETH_zeros = []\r\n",
        "time_ETH_ones = []\r\n",
        "\r\n",
        "# for converting miliseconds to seconds in the first raws it is needed to divide values by 1000\r\n",
        "\r\n",
        "for i in range(dataset_ETH.shape[1]):\r\n",
        "  if dataset_ETH[1,i] == 0:\r\n",
        "    time_ETH_zeros.append(dataset_ETH[0,i]/1000)\r\n",
        "  elif dataset_ETH[1,i] == 1:\r\n",
        "    time_ETH_ones.append(dataset_ETH[0,i]/1000)\r\n",
        "\r\n",
        "line_zeros = np.zeros_like(time_ETH_zeros)\r\n",
        "line_ones = np.ones_like(time_ETH_ones)\r\n",
        "\r\n",
        "dataset_zeros = np.array([time_ETH_zeros, line_zeros])\r\n",
        "dataset_ones = np.array([time_ETH_ones, line_ones])\r\n",
        "\r\n",
        "print(dataset_zeros)\r\n",
        "print()\r\n",
        "print(dataset_ones)\r\n",
        "\r\n",
        "dataset_zeros.shape\r\n",
        "dataset_ones.shape\r\n",
        "\r\n",
        "# now we need to divide prepared data into train:val:test = 60:20:20\r\n",
        "proportion_zeros = [int(0.6 * dataset_zeros.shape[1]), int(0.8 * dataset_zeros.shape[1])]\r\n",
        "proportion_ones = [int(0.6 * dataset_ones.shape[1]), int(0.8 * dataset_ones.shape[1])]\r\n",
        "\r\n",
        "# cut zeros dataset into the train, val and test\r\n",
        "dataset_zeros_split = np.hsplit(dataset_zeros, proportion_zeros)\r\n",
        "dataset_ones_split = np.hsplit(dataset_ones, proportion_ones)\r\n",
        "num_seq = 3319 # num of sequences in every future batch\r\n",
        "\r\n",
        "# FOR ZEROS\r\n",
        "for part_idx, part in enumerate(dataset_zeros_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_zeros = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_zeros_cut = np.hsplit(part, cut_zeros)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_zeros_cut.pop(0)\r\n",
        "  dataset_zeros_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_zeros_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/train_ETH/train_ETH_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/val_ETH/val_ETH_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/test_ETH/test_ETH_zeros_' + str(cut_idx) + '.npy', cut)\r\n",
        "\r\n",
        "# FOR ONES\r\n",
        "for part_idx, part in enumerate(dataset_ones_split):\r\n",
        "  print(part_idx, part)\r\n",
        "  \r\n",
        "  # cut each part (train, val, test) into the 'num_seq' length\r\n",
        "  cut_ones = [i for i in range(0, part.shape[1], num_seq)]\r\n",
        "  dataset_ones_cut = np.hsplit(part, cut_ones)\r\n",
        "  # removing unneeded stuff\r\n",
        "  dataset_ones_cut.pop(0)\r\n",
        "  dataset_ones_cut.pop(-1)\r\n",
        "\r\n",
        "  # recording all the values \r\n",
        "  for cut_idx, cut in enumerate(dataset_ones_cut):\r\n",
        "    if part_idx == 0:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/train_ETH/train_ETH_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 1:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/val_ETH/val_ETH_ones_' + str(cut_idx) + '.npy', cut)\r\n",
        "    if part_idx == 2:\r\n",
        "      np.save('/content/drive/MyDrive/unipoint/data/data_ETH/test_ETH/test_ETH_ones_' + str(cut_idx) + '.npy', cut)\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[13.389 27.387 11.675 ...  9.016 21.031  5.295]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "\n",
            "[[14.272 12.05  16.608 ... 12.158  9.347  8.678]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "0 [[13.389 27.387 11.675 ... 34.997 13.486 16.646]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "1 [[21.735 31.121 14.594 ... 20.837 69.819 22.316]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "2 [[25.528 25.284  6.598 ...  9.016 21.031  5.295]\n",
            " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
            "0 [[14.272 12.05  16.608 ... 12.433 27.342 13.361]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "1 [[25.574  8.531 39.553 ... 25.434 44.177 19.299]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n",
            "2 [[46.535 33.063 26.747 ... 12.158  9.347  8.678]\n",
            " [ 1.     1.     1.    ...  1.     1.     1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMNT7-QdXQ-H"
      },
      "source": [
        "train_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_ETH/train_ETH/')\r\n",
        "val_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_ETH/val_ETH/')\r\n",
        "test_dataset = TupoDataset('/content/drive/MyDrive/unipoint/data/data_ETH/test_ETH/')\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=12)\r\n",
        "val_loader = DataLoader(val_dataset, batch_size=12)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=12)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF6sA5kfXHJh",
        "outputId": "4725f88f-c65a-4ba4-8e20-44e8d2317f57"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 6.2000,  5.7620, 15.8070,  ...,  8.8620,  6.9280,  9.0080]),\n",
              " tensor([0, 0, 0,  ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_0Y0XSGlyJP"
      },
      "source": [
        "## Model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB3IwNmFUM1I"
      },
      "source": [
        "def create_unifrom_d(event_times, device = None):\r\n",
        "    \"\"\"\r\n",
        "    Create uniform distribution of t from given event sequenses\r\n",
        "    Inputs:\r\n",
        "        event_times (B, T) - inter-arrival times of events\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    batch_size, batch_len = event_times.shape\r\n",
        "    sim_inter_times = []\r\n",
        "    tot_time_seqs = event_times.sum(dim=1)\r\n",
        "    for tot_time in tot_time_seqs:\r\n",
        "\r\n",
        "          sim_time_seqs = torch.zeros(batch_len).uniform_(0,tot_time)\r\n",
        "          sim_inter_time = torch.zeros(batch_len)\r\n",
        "          sim_inter_time[1:] = abs(sim_time_seqs[1:] - sim_time_seqs[:-1])\r\n",
        "          sim_inter_times.append(sim_inter_time)\r\n",
        "\r\n",
        "    if device != None:\r\n",
        "        sim_inter_times = torch.stack(sim_inter_times).to(device)\r\n",
        "    else: \r\n",
        "        sim_inter_times = torch.stack(sim_inter_times)\r\n",
        "\r\n",
        "    return sim_inter_times"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zef5MhBt6Ilk"
      },
      "source": [
        "class UNIPoint(nn.Module):\r\n",
        "    def __init__(self, n_features, n_parameters, n_basis_functions, device, hidden_size=256):\r\n",
        "      \"\"\"\r\n",
        "      Input parameters:\r\n",
        "      n_neurons - number of neurons inside RNN\r\n",
        "      n_parameters - expecteed number of parameters in basis function\r\n",
        "      n_basis_functions - number of basis functions\r\n",
        "      \"\"\"\r\n",
        "      super(UNIPoint, self).__init__()\r\n",
        "\r\n",
        "      #self.rnn = nn.RNNCell(n_features, hidden_size) # uncomment if RNN\r\n",
        "      self.rnn = nn.LSTMCell(n_features, hidden_size) # uncomment if LSTM\r\n",
        "      self.h2p = nn.Linear(hidden_size, n_parameters * n_basis_functions)\r\n",
        "      self.Softplus = torch.nn.Softplus(beta = 1)\r\n",
        "\r\n",
        "      self.n_basis_functions = n_basis_functions\r\n",
        "      self.hidden_size = hidden_size\r\n",
        "      self.device = device\r\n",
        "\r\n",
        "      self.time_predictor  = nn.Linear(hidden_size, 1, bias=False) #here 12 is a batch_size - fix later\r\n",
        "\r\n",
        "    def ReLU(self, parameter_1, parameter_2, time):\r\n",
        "      \"\"\"Function to apply Rectified Linear Unit (ReLU) as basis function inside network \r\n",
        "        Input parameters:\r\n",
        "          parameters - alpha, beta for basis function's value calculation\r\n",
        "          time - column-vector with time which had been spent since the begining of \r\n",
        "                  temporal point process (TPP)\r\n",
        "      \"\"\"\r\n",
        "      self.output = torch.relu(self.parameters[:,parameter_1] * time + self.parameters[:,parameter_2] ) \r\n",
        "      return self.output\r\n",
        "    \r\n",
        "    def PowerLaw(self, parameter_1, parameter_2, time): \r\n",
        "      \"\"\"Function to apply Power Law (PL) as basis function inside network \r\n",
        "        Input parameters:\r\n",
        "          parameters - alpha, beta for basis function's value calculation\r\n",
        "          time - column-vector with time which had been spent since the begining of \r\n",
        "                  temporal point process (TPP)\r\n",
        "      \"\"\"\r\n",
        "      self.output = self.parameters[:,parameter_1] * (1 + time)**( - self.parameters[:,parameter_2])\r\n",
        "      return self.output\r\n",
        "\r\n",
        "    def Exponential(self, parameter_1, parameter_2, time): \r\n",
        "      \"\"\"Function to apply Exponential function as basis function inside network \r\n",
        "        Input parameters:\r\n",
        "          parameters - alpha, beta for basis function's value calculation\r\n",
        "          time - column-vector with time which had been spent since the begining of \r\n",
        "                  temporal point process (TPP)\r\n",
        "      \"\"\"\r\n",
        "      self.output = self.parameters[:,parameter_1] * torch.exp(self.parameters[:, parameter_2] * time)\r\n",
        "      return self.output\r\n",
        "\r\n",
        "\r\n",
        "    def intensity_layer(self, tau):\r\n",
        "          '''\r\n",
        "          Layer to calculate intesity with respect to time from the last event\r\n",
        "\r\n",
        "          Input: tau - time from the last event\r\n",
        "          '''\r\n",
        "\r\n",
        "          for function in range(self.n_basis_functions): \r\n",
        "              # calculating numbers of parameters to take for basis function\r\n",
        "              par1 = 2 * function\r\n",
        "              par2 = 2 * function + 1\r\n",
        "              self.basis_res[:, function] = self.ReLU(par1, par2, tau) \r\n",
        "          \r\n",
        "          self.sum_res = torch.sum(self.basis_res, 1)\r\n",
        "          intensity = self.Softplus(self.sum_res)\r\n",
        "\r\n",
        "          return intensity\r\n",
        "\r\n",
        "    def init_hidden(self, batch_size, hidden_size):\r\n",
        "\r\n",
        "      self.hx = torch.randn(batch_size, hidden_size, device=self.device) # initialize hidden state \r\n",
        "      self.basis_res = torch.randn(batch_size, self.n_basis_functions) #initialize matrix for basis f-s calculations results\r\n",
        "\r\n",
        "      self.cx = torch.randn(batch_size, hidden_size, device=self.device) # initialize cell state (for LSTM only)\r\n",
        "\r\n",
        "    def forward(self, event_times, event_type):\r\n",
        "      \"\"\"Input parameters:\r\n",
        "          event_times - interarrival times between events\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "        \r\n",
        "      hidden_states, intensity_values = [], []\r\n",
        "      batch_size, batch_len = event_times.shape\r\n",
        "\r\n",
        "      # init hidden states\r\n",
        "      self.init_hidden(batch_size, self.hidden_size)\r\n",
        "\r\n",
        "      # for each time step (here X shape is (batch_size, seq_len, n_features) )\r\n",
        "      for i in range(batch_len):\r\n",
        "\r\n",
        "          #self.hx = self.rnn(event_times[:,i].reshape(-1,1), self.hx) # uncomment if you use RNN\r\n",
        "          self.hx, self.cx = self.rnn(event_times[:,i].reshape(-1,1), (self.hx, self.cx)) # uncomment if you use LSTM\r\n",
        "          self.parameters = self.h2p(self.hx)\r\n",
        "          \r\n",
        "          intensity = self.intensity_layer(event_times[:,i])\r\n",
        "          hidden_states.append(self.hx)\r\n",
        "          intensity_values.append(intensity)\r\n",
        "\r\n",
        "      # make predictions\r\n",
        "      #print(\"'intensity_values' length \", len(intensity_values))\r\n",
        "      #print(\"'torch.stack(intensity_values)' shape is \", torch.stack(intensity_values).shape)\r\n",
        "      #stack_intensity.append(torch.stack(intensity_values))\r\n",
        "      time_pred  = self.time_predict(batch_size, hidden_states)\r\n",
        "                    \r\n",
        "      return  torch.stack(intensity_values), time_pred\r\n",
        "\r\n",
        "    def LogLikelihoodLoss(self, intensity, event_times):\r\n",
        "        \"\"\"\r\n",
        "        Inputs:\r\n",
        "            intensity (S, B) - intensity values,\r\n",
        "            event_times (B, S) - inter-arrival times of events\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # Compute log-likelihood of of the events that happened (first term) via sum of log-intensities \r\n",
        "        original_loglikelihood = intensity.log().sum(dim=0)\r\n",
        "\r\n",
        "        #Compute log-probabilities of non-events (second term) using Monte Carlo method\r\n",
        "\r\n",
        "        #Calc intensity of simulated events\r\n",
        "        sim_times = create_unifrom_d(event_times, self.device)\r\n",
        "        sim_intesity = []\r\n",
        "        for i in range(sim_times.shape[1]):\r\n",
        "            sim_intesity.append(self.intensity_layer(sim_times[:,i]))\r\n",
        "\r\n",
        "        sim_intesity = torch.stack(sim_intesity).to(self.device)\r\n",
        "        tot_time_seqs, seq_len = event_times.sum(dim=1), event_times.shape[1]\r\n",
        "        mc_coef = (tot_time_seqs / seq_len)\r\n",
        "\r\n",
        "        simulated_likelihood = sim_intesity.sum(dim=0) * mc_coef\r\n",
        "        \r\n",
        "        # sum over batch\r\n",
        "        LLH = (original_loglikelihood - simulated_likelihood).sum()\r\n",
        "\r\n",
        "        return -LLH\r\n",
        "\r\n",
        "    def time_predict(self, batch_size, hidden_states):\r\n",
        "        # output prediction layer\r\n",
        "        #print(\" In time predict function length of hidden_states is \", torch.stack(hidden_states).shape)\r\n",
        "        time_prediction = self.time_predictor(torch.stack(hidden_states))\r\n",
        "        \r\n",
        "        return time_prediction\r\n",
        "\r\n",
        "\r\n",
        "    def time_error(self, time_pred, time):\r\n",
        "        \"\"\"\r\n",
        "        Function to compute mean squared error for time predictions.\r\n",
        "        Input:\r\n",
        "            time_pred (B, S) - time predictions,\r\n",
        "            time (B, S) - ground truth for times\r\n",
        "        Output:\r\n",
        "            time_error (float) - time prediction error for the whole batch\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        time_ground_truth = time[:, 1:] # - time[:, :-1]\r\n",
        "        time_pred = time_pred[:-1, :]\r\n",
        "\r\n",
        "        time_error = nn.MSELoss(reduction='mean')(time_pred, time_ground_truth)\r\n",
        "        return time_error"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ox7WeI9BZm"
      },
      "source": [
        "# model evaluation\r\n",
        "\r\n",
        "# X_batch dimension = (batch_size, seq_len, n_features)\r\n",
        "#BATCH_SIZE = 256\r\n",
        "N_FEATURES = 1\r\n",
        "N_PARAMETERS = 2\r\n",
        "N_BASIS_FUNCTIONS = 1\r\n",
        "\r\n",
        "model = UNIPoint(N_FEATURES, N_PARAMETERS, N_BASIS_FUNCTIONS, device).to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCWGTW1phYym",
        "outputId": "571de475-0031-4ec7-ce50-89f05765e575"
      },
      "source": [
        "epoch_list = []\r\n",
        "loss_list, loss_list_val = [], []\r\n",
        "mse_sum_list, mse_sum_list_val = [], []\r\n",
        "\r\n",
        "time0 = time()\r\n",
        "\r\n",
        "best_loss_mse_val = 10e10\r\n",
        "best_loss_llh_val = -10e10\r\n",
        "best_epoch = 0\r\n",
        "\r\n",
        "for epoch in range(60):\r\n",
        "    epoch_ll, event_num = 0, 0\r\n",
        "    epoch_ll_val, event_num_val = 0, 0\r\n",
        "\r\n",
        "    for inter_time, event in train_loader:\r\n",
        "\r\n",
        "        intensity_values, time_pred = model(inter_time.to(device), event.to(device))\r\n",
        "        LLH_loss = model.LogLikelihoodLoss(intensity_values.to(device), inter_time.to(device)).mean()\r\n",
        "        \r\n",
        "        #print('time_pred', time_pred)\r\n",
        "        #print('time_pred shape ', time_pred.shape)\r\n",
        "        #print(\"torch.mean(inter_time,0).view(-1,1)\", torch.mean(inter_time, 0).view(-1,1))\r\n",
        "        #print('torch.mean(inter_time,0).view(-1,1) shape is ', torch.mean(inter_time, 0).view(-1,1).shape)\r\n",
        "        #print()\r\n",
        "\r\n",
        "        # Time prediction loss\r\n",
        "        #loss_tp = model.time_error(time_pred.to(device), torch.mean(inter_time, 0).view(-1,1).to(device))\r\n",
        "        loss_tp = model.time_error(time_pred.to(device), inter_time.to(device))\r\n",
        "        \r\n",
        "        loss = loss_tp + LLH_loss\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward() \r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        event_num += inter_time.shape[0] * inter_time.shape[1]\r\n",
        "        epoch_ll += LLH_loss.detach().cpu().item()\r\n",
        "\r\n",
        "    # Validation check\r\n",
        "    with torch.no_grad():\r\n",
        "      for inter_time, event in val_loader:\r\n",
        "        \r\n",
        "        intensity_values, time_pred = model(inter_time.to(device), event.to(device))\r\n",
        "        \r\n",
        "        LLH_loss_val = model.LogLikelihoodLoss(intensity_values.to(device), inter_time.to(device)).mean()\r\n",
        "        loss_tp_val = model.time_error(time_pred.to(device), inter_time.to(device))\r\n",
        "        \r\n",
        "        loss_val = loss_tp_val + LLH_loss_val\r\n",
        "\r\n",
        "        event_num_val += inter_time.shape[0] * inter_time.shape[1]\r\n",
        "        epoch_ll_val += LLH_loss_val.detach().cpu().item()\r\n",
        "\r\n",
        "\r\n",
        "    loss_list.append(-epoch_ll/event_num)\r\n",
        "    loss_list_val.append(-epoch_ll_val/event_num_val)\r\n",
        "\r\n",
        "    mse_sum_list.append(loss_tp)\r\n",
        "    mse_sum_list_val.append(loss_tp_val)\r\n",
        "    \r\n",
        "    epoch_list.append(epoch)\r\n",
        "\r\n",
        "    print('EPOCH:', epoch)\r\n",
        "    \r\n",
        "    print('TRAINING: ')\r\n",
        "    print('LogLike/nats', -epoch_ll/event_num, 'Time loss ', loss_tp)\r\n",
        "    \r\n",
        "    print('VALIDATION: ')\r\n",
        "    print('LogLike/nats', -epoch_ll_val/event_num_val, 'Time loss ', loss_tp_val)\r\n",
        "\r\n",
        "    if (loss_tp_val <= best_loss_mse_val): #(-epoch_ll_val/event_num_val >= best_loss_llh_val) and :\r\n",
        "      torch.save(model.state_dict(), 'UNIPoint_model_epoch_' + str(epoch) + '_PL_' + str(N_BASIS_FUNCTIONS) + '.pth')\r\n",
        "      best_loss_llh_val = -epoch_ll_val/event_num_val\r\n",
        "      best_loss_mse_val = loss_tp_val\r\n",
        "      best_epoch = epoch\r\n",
        "    \r\n",
        "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\r\n",
        "    print(\"BEST RESULTS: LLH:\", best_loss_llh_val, \", MSE: \", best_loss_mse_val, \"Epoch: \", best_epoch)\r\n",
        "\r\n",
        "    print('-'*60)\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([12, 3318])) that is different to the input size (torch.Size([3318, 12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([9, 3318])) that is different to the input size (torch.Size([3318, 9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([11, 3318])) that is different to the input size (torch.Size([3318, 11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n",
            "TRAINING: \n",
            "LogLike/nats -18297.780302868123 Time loss  tensor(110.3069, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811945279174132 Time loss  tensor(308.8614, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 2.2270918210347492\n",
            "BEST RESULTS: LLH: -14.811945279174132 , MSE:  tensor(308.8614, device='cuda:0') Epoch:  0\n",
            "------------------------------------------------------------\n",
            "EPOCH: 1\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622979469613 Time loss  tensor(137.3652, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949937362236 Time loss  tensor(268.9155, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 4.462792229652405\n",
            "BEST RESULTS: LLH: -14.811949937362236 , MSE:  tensor(268.9155, device='cuda:0') Epoch:  1\n",
            "------------------------------------------------------------\n",
            "EPOCH: 2\n",
            "TRAINING: \n",
            "LogLike/nats -11.887619725345301 Time loss  tensor(136.5342, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953901777644 Time loss  tensor(268.8182, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 6.713307511806488\n",
            "BEST RESULTS: LLH: -14.811953901777644 , MSE:  tensor(268.8182, device='cuda:0') Epoch:  2\n",
            "------------------------------------------------------------\n",
            "EPOCH: 3\n",
            "TRAINING: \n",
            "LogLike/nats -11.887625060787704 Time loss  tensor(130.7786, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948351596074 Time loss  tensor(269.3138, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 8.958764064311982\n",
            "BEST RESULTS: LLH: -14.811953901777644 , MSE:  tensor(268.8182, device='cuda:0') Epoch:  2\n",
            "------------------------------------------------------------\n",
            "EPOCH: 4\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621311111463 Time loss  tensor(131.1599, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950532024548 Time loss  tensor(269.0305, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 11.198163072268168\n",
            "BEST RESULTS: LLH: -14.811953901777644 , MSE:  tensor(268.8182, device='cuda:0') Epoch:  2\n",
            "------------------------------------------------------------\n",
            "EPOCH: 5\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762654744348 Time loss  tensor(131.4209, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949540920695 Time loss  tensor(268.7841, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 13.466383655865988\n",
            "BEST RESULTS: LLH: -14.811949540920695 , MSE:  tensor(268.7841, device='cuda:0') Epoch:  5\n",
            "------------------------------------------------------------\n",
            "EPOCH: 6\n",
            "TRAINING: \n",
            "LogLike/nats -11.887620551265178 Time loss  tensor(131.2111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950333803777 Time loss  tensor(268.6117, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 15.712935674190522\n",
            "BEST RESULTS: LLH: -14.811950333803777 , MSE:  tensor(268.6117, device='cuda:0') Epoch:  6\n",
            "------------------------------------------------------------\n",
            "EPOCH: 7\n",
            "TRAINING: \n",
            "LogLike/nats -11.887624333978211 Time loss  tensor(131.0629, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.8119515231284 Time loss  tensor(268.4093, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 17.96149382988612\n",
            "BEST RESULTS: LLH: -14.8119515231284 , MSE:  tensor(268.4093, device='cuda:0') Epoch:  7\n",
            "------------------------------------------------------------\n",
            "EPOCH: 8\n",
            "TRAINING: \n",
            "LogLike/nats -11.887624499162186 Time loss  tensor(130.9490, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811954199108799 Time loss  tensor(268.2289, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 20.217923033237458\n",
            "BEST RESULTS: LLH: -14.811954199108799 , MSE:  tensor(268.2289, device='cuda:0') Epoch:  8\n",
            "------------------------------------------------------------\n",
            "EPOCH: 9\n",
            "TRAINING: \n",
            "LogLike/nats -11.887625457229243 Time loss  tensor(130.8093, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811952514232251 Time loss  tensor(268.0099, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 22.474482015768686\n",
            "BEST RESULTS: LLH: -14.811952514232251 , MSE:  tensor(268.0099, device='cuda:0') Epoch:  9\n",
            "------------------------------------------------------------\n",
            "EPOCH: 10\n",
            "TRAINING: \n",
            "LogLike/nats -11.88761691721772 Time loss  tensor(130.6833, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953108894562 Time loss  tensor(267.8415, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 24.725640420118967\n",
            "BEST RESULTS: LLH: -14.811953108894562 , MSE:  tensor(267.8415, device='cuda:0') Epoch:  10\n",
            "------------------------------------------------------------\n",
            "EPOCH: 11\n",
            "TRAINING: \n",
            "LogLike/nats -11.887620303489214 Time loss  tensor(130.5590, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950928466088 Time loss  tensor(267.6450, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 26.968314508597057\n",
            "BEST RESULTS: LLH: -14.811950928466088 , MSE:  tensor(267.6450, device='cuda:0') Epoch:  11\n",
            "------------------------------------------------------------\n",
            "EPOCH: 12\n",
            "TRAINING: \n",
            "LogLike/nats -11.887624565235777 Time loss  tensor(130.4316, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953208004947 Time loss  tensor(267.4651, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 29.209840190410613\n",
            "BEST RESULTS: LLH: -14.811953208004947 , MSE:  tensor(267.4651, device='cuda:0') Epoch:  12\n",
            "------------------------------------------------------------\n",
            "EPOCH: 13\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622021402557 Time loss  tensor(130.2868, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947162271451 Time loss  tensor(267.2686, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 31.49268959760666\n",
            "BEST RESULTS: LLH: -14.811947162271451 , MSE:  tensor(267.2686, device='cuda:0') Epoch:  13\n",
            "------------------------------------------------------------\n",
            "EPOCH: 14\n",
            "TRAINING: \n",
            "LogLike/nats -11.887624895603727 Time loss  tensor(130.1596, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811938638778326 Time loss  tensor(267.0878, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 33.76497049729029\n",
            "BEST RESULTS: LLH: -14.811938638778326 , MSE:  tensor(267.0878, device='cuda:0') Epoch:  14\n",
            "------------------------------------------------------------\n",
            "EPOCH: 15\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621624961017 Time loss  tensor(130.0328, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948351596074 Time loss  tensor(266.9057, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 36.03169449965159\n",
            "BEST RESULTS: LLH: -14.811948351596074 , MSE:  tensor(266.9057, device='cuda:0') Epoch:  15\n",
            "------------------------------------------------------------\n",
            "EPOCH: 16\n",
            "TRAINING: \n",
            "LogLike/nats -11.887612985839109 Time loss  tensor(129.9213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949342699926 Time loss  tensor(266.7239, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 38.29739744265874\n",
            "BEST RESULTS: LLH: -14.811949342699926 , MSE:  tensor(266.7239, device='cuda:0') Epoch:  16\n",
            "------------------------------------------------------------\n",
            "EPOCH: 17\n",
            "TRAINING: \n",
            "LogLike/nats -11.887624333978211 Time loss  tensor(129.7811, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811942801414503 Time loss  tensor(266.5544, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 40.53306432167689\n",
            "BEST RESULTS: LLH: -14.811942801414503 , MSE:  tensor(266.5544, device='cuda:0') Epoch:  17\n",
            "------------------------------------------------------------\n",
            "EPOCH: 18\n",
            "TRAINING: \n",
            "LogLike/nats -11.887618882907027 Time loss  tensor(129.6528, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953108894562 Time loss  tensor(266.3656, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 42.782494095961255\n",
            "BEST RESULTS: LLH: -14.811953108894562 , MSE:  tensor(266.3656, device='cuda:0') Epoch:  18\n",
            "------------------------------------------------------------\n",
            "EPOCH: 19\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623590650323 Time loss  tensor(129.5377, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948748037615 Time loss  tensor(266.1785, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 45.03154520988464\n",
            "BEST RESULTS: LLH: -14.811948748037615 , MSE:  tensor(266.1785, device='cuda:0') Epoch:  19\n",
            "------------------------------------------------------------\n",
            "EPOCH: 20\n",
            "TRAINING: \n",
            "LogLike/nats -11.8876237393159 Time loss  tensor(129.4021, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811952018680325 Time loss  tensor(266.0345, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 47.29141749938329\n",
            "BEST RESULTS: LLH: -14.811952018680325 , MSE:  tensor(266.0345, device='cuda:0') Epoch:  20\n",
            "------------------------------------------------------------\n",
            "EPOCH: 21\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762314465359 Time loss  tensor(129.2853, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.81193606190831 Time loss  tensor(265.8618, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 49.563737006982166\n",
            "BEST RESULTS: LLH: -14.81193606190831 , MSE:  tensor(265.8618, device='cuda:0') Epoch:  21\n",
            "------------------------------------------------------------\n",
            "EPOCH: 22\n",
            "TRAINING: \n",
            "LogLike/nats -11.887626266630722 Time loss  tensor(129.1579, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950631134932 Time loss  tensor(265.6893, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 51.82668855190277\n",
            "BEST RESULTS: LLH: -14.811950631134932 , MSE:  tensor(265.6893, device='cuda:0') Epoch:  22\n",
            "------------------------------------------------------------\n",
            "EPOCH: 23\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621360666657 Time loss  tensor(129.0491, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.81195459555034 Time loss  tensor(265.5296, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 54.08468836943309\n",
            "BEST RESULTS: LLH: -14.81195459555034 , MSE:  tensor(265.5296, device='cuda:0') Epoch:  23\n",
            "------------------------------------------------------------\n",
            "EPOCH: 24\n",
            "TRAINING: \n",
            "LogLike/nats -11.887626184038735 Time loss  tensor(128.9292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950135583007 Time loss  tensor(265.3692, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 56.36024810473124\n",
            "BEST RESULTS: LLH: -14.811950135583007 , MSE:  tensor(265.3692, device='cuda:0') Epoch:  24\n",
            "------------------------------------------------------------\n",
            "EPOCH: 25\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762667959066 Time loss  tensor(128.8060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.8119515231284 Time loss  tensor(265.2059, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 58.649386032422385\n",
            "BEST RESULTS: LLH: -14.8119515231284 , MSE:  tensor(265.2059, device='cuda:0') Epoch:  25\n",
            "------------------------------------------------------------\n",
            "EPOCH: 26\n",
            "TRAINING: \n",
            "LogLike/nats -11.887625572858026 Time loss  tensor(128.6926, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.8119515231284 Time loss  tensor(265.0614, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 60.926627441247305\n",
            "BEST RESULTS: LLH: -14.8119515231284 , MSE:  tensor(265.0614, device='cuda:0') Epoch:  26\n",
            "------------------------------------------------------------\n",
            "EPOCH: 27\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622995988012 Time loss  tensor(128.5807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.81194676582991 Time loss  tensor(264.8865, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 63.20984069108963\n",
            "BEST RESULTS: LLH: -14.81194676582991 , MSE:  tensor(264.8865, device='cuda:0') Epoch:  27\n",
            "------------------------------------------------------------\n",
            "EPOCH: 28\n",
            "TRAINING: \n",
            "LogLike/nats -11.887628331430413 Time loss  tensor(128.4627, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949739141467 Time loss  tensor(264.7514, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 65.50593969027202\n",
            "BEST RESULTS: LLH: -14.811949739141467 , MSE:  tensor(264.7514, device='cuda:0') Epoch:  28\n",
            "------------------------------------------------------------\n",
            "EPOCH: 29\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621178964283 Time loss  tensor(128.3614, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950928466088 Time loss  tensor(264.5993, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 67.7562305688858\n",
            "BEST RESULTS: LLH: -14.811950928466088 , MSE:  tensor(264.5993, device='cuda:0') Epoch:  29\n",
            "------------------------------------------------------------\n",
            "EPOCH: 30\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622120512942 Time loss  tensor(128.2495, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953009784178 Time loss  tensor(264.4492, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 70.02366010745367\n",
            "BEST RESULTS: LLH: -14.811953009784178 , MSE:  tensor(264.4492, device='cuda:0') Epoch:  30\n",
            "------------------------------------------------------------\n",
            "EPOCH: 31\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623904499875 Time loss  tensor(128.1397, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950135583007 Time loss  tensor(264.3183, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 72.28525249958038\n",
            "BEST RESULTS: LLH: -14.811950135583007 , MSE:  tensor(264.3183, device='cuda:0') Epoch:  31\n",
            "------------------------------------------------------------\n",
            "EPOCH: 32\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623441984745 Time loss  tensor(128.0306, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.8119515231284 Time loss  tensor(264.1721, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 74.54260822534562\n",
            "BEST RESULTS: LLH: -14.8119515231284 , MSE:  tensor(264.1721, device='cuda:0') Epoch:  32\n",
            "------------------------------------------------------------\n",
            "EPOCH: 33\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762233525211 Time loss  tensor(127.9184, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949937362236 Time loss  tensor(264.0256, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 76.80952157179514\n",
            "BEST RESULTS: LLH: -14.811949937362236 , MSE:  tensor(264.0256, device='cuda:0') Epoch:  33\n",
            "------------------------------------------------------------\n",
            "EPOCH: 34\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621591924221 Time loss  tensor(127.8230, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.81194676582991 Time loss  tensor(263.9046, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 79.08097685178122\n",
            "BEST RESULTS: LLH: -14.81194676582991 , MSE:  tensor(263.9046, device='cuda:0') Epoch:  34\n",
            "------------------------------------------------------------\n",
            "EPOCH: 35\n",
            "TRAINING: \n",
            "LogLike/nats -11.8876264152963 Time loss  tensor(127.7056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811951324907628 Time loss  tensor(263.7676, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 81.28957773049673\n",
            "BEST RESULTS: LLH: -14.811951324907628 , MSE:  tensor(263.7676, device='cuda:0') Epoch:  35\n",
            "------------------------------------------------------------\n",
            "EPOCH: 36\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623508058335 Time loss  tensor(127.6108, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948054264919 Time loss  tensor(263.6406, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 83.48484528462092\n",
            "BEST RESULTS: LLH: -14.811948054264919 , MSE:  tensor(263.6406, device='cuda:0') Epoch:  36\n",
            "------------------------------------------------------------\n",
            "EPOCH: 37\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623309837565 Time loss  tensor(127.5070, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811951324907628 Time loss  tensor(263.5160, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 85.71583970387776\n",
            "BEST RESULTS: LLH: -14.811951324907628 , MSE:  tensor(263.5160, device='cuda:0') Epoch:  37\n",
            "------------------------------------------------------------\n",
            "EPOCH: 38\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621245037874 Time loss  tensor(127.4112, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947955154533 Time loss  tensor(263.3701, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 87.92815797328949\n",
            "BEST RESULTS: LLH: -14.811947955154533 , MSE:  tensor(263.3701, device='cuda:0') Epoch:  38\n",
            "------------------------------------------------------------\n",
            "EPOCH: 39\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762207095775 Time loss  tensor(127.3027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.81195439732957 Time loss  tensor(263.2572, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 90.05496912002563\n",
            "BEST RESULTS: LLH: -14.81195439732957 , MSE:  tensor(263.2572, device='cuda:0') Epoch:  39\n",
            "------------------------------------------------------------\n",
            "EPOCH: 40\n",
            "TRAINING: \n",
            "LogLike/nats -11.887627274252972 Time loss  tensor(127.2135, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947856044148 Time loss  tensor(263.1506, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 92.13106845617294\n",
            "BEST RESULTS: LLH: -14.811947856044148 , MSE:  tensor(263.1506, device='cuda:0') Epoch:  40\n",
            "------------------------------------------------------------\n",
            "EPOCH: 41\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622979469613 Time loss  tensor(127.1051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811945675615673 Time loss  tensor(263.0165, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 94.22748197714488\n",
            "BEST RESULTS: LLH: -14.811945675615673 , MSE:  tensor(263.0165, device='cuda:0') Epoch:  41\n",
            "------------------------------------------------------------\n",
            "EPOCH: 42\n",
            "TRAINING: \n",
            "LogLike/nats -11.887620171342034 Time loss  tensor(127.0142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811942603193733 Time loss  tensor(262.9040, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 96.2951643427213\n",
            "BEST RESULTS: LLH: -14.811942603193733 , MSE:  tensor(262.9040, device='cuda:0') Epoch:  42\n",
            "------------------------------------------------------------\n",
            "EPOCH: 43\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622236141725 Time loss  tensor(126.9267, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811952216901096 Time loss  tensor(262.7798, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 98.34332062403361\n",
            "BEST RESULTS: LLH: -14.811952216901096 , MSE:  tensor(262.7798, device='cuda:0') Epoch:  43\n",
            "------------------------------------------------------------\n",
            "EPOCH: 44\n",
            "TRAINING: \n",
            "LogLike/nats -11.887626514406685 Time loss  tensor(126.8370, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950036472622 Time loss  tensor(262.6937, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 100.41332391500472\n",
            "BEST RESULTS: LLH: -14.811950036472622 , MSE:  tensor(262.6937, device='cuda:0') Epoch:  44\n",
            "------------------------------------------------------------\n",
            "EPOCH: 45\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621492813837 Time loss  tensor(126.7430, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953802667258 Time loss  tensor(262.5583, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 102.50889172554017\n",
            "BEST RESULTS: LLH: -14.811953802667258 , MSE:  tensor(262.5583, device='cuda:0') Epoch:  45\n",
            "------------------------------------------------------------\n",
            "EPOCH: 46\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622483917688 Time loss  tensor(126.6519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811949540920695 Time loss  tensor(262.5253, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 104.57399261792501\n",
            "BEST RESULTS: LLH: -14.811949540920695 , MSE:  tensor(262.5253, device='cuda:0') Epoch:  46\n",
            "------------------------------------------------------------\n",
            "EPOCH: 47\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621261556271 Time loss  tensor(126.5718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811952910673792 Time loss  tensor(262.3531, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 106.62762970129648\n",
            "BEST RESULTS: LLH: -14.811952910673792 , MSE:  tensor(262.3531, device='cuda:0') Epoch:  47\n",
            "------------------------------------------------------------\n",
            "EPOCH: 48\n",
            "TRAINING: \n",
            "LogLike/nats -11.88761586004028 Time loss  tensor(126.4910, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947360492221 Time loss  tensor(262.2423, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 108.67534180084864\n",
            "BEST RESULTS: LLH: -14.811947360492221 , MSE:  tensor(262.2423, device='cuda:0') Epoch:  48\n",
            "------------------------------------------------------------\n",
            "EPOCH: 49\n",
            "TRAINING: \n",
            "LogLike/nats -11.887623062061602 Time loss  tensor(126.3949, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947657823378 Time loss  tensor(262.1367, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 110.72823590437571\n",
            "BEST RESULTS: LLH: -14.811947657823378 , MSE:  tensor(262.1367, device='cuda:0') Epoch:  49\n",
            "------------------------------------------------------------\n",
            "EPOCH: 50\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762481301174 Time loss  tensor(126.3188, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811952910673792 Time loss  tensor(262.0474, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 112.81047653357187\n",
            "BEST RESULTS: LLH: -14.811952910673792 , MSE:  tensor(262.0474, device='cuda:0') Epoch:  50\n",
            "------------------------------------------------------------\n",
            "EPOCH: 51\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762494515892 Time loss  tensor(126.2242, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948847147999 Time loss  tensor(261.9336, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 114.9338576634725\n",
            "BEST RESULTS: LLH: -14.811948847147999 , MSE:  tensor(261.9336, device='cuda:0') Epoch:  51\n",
            "------------------------------------------------------------\n",
            "EPOCH: 52\n",
            "TRAINING: \n",
            "LogLike/nats -11.887622913396024 Time loss  tensor(126.1386, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948549816844 Time loss  tensor(261.8435, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 117.04772149324417\n",
            "BEST RESULTS: LLH: -14.811948549816844 , MSE:  tensor(261.8435, device='cuda:0') Epoch:  52\n",
            "------------------------------------------------------------\n",
            "EPOCH: 53\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762588670758 Time loss  tensor(126.0715, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950928466088 Time loss  tensor(261.7676, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 119.20077375968297\n",
            "BEST RESULTS: LLH: -14.811950928466088 , MSE:  tensor(261.7676, device='cuda:0') Epoch:  53\n",
            "------------------------------------------------------------\n",
            "EPOCH: 54\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621691034607 Time loss  tensor(125.9884, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811948450706458 Time loss  tensor(261.7383, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 121.37644979556401\n",
            "BEST RESULTS: LLH: -14.811948450706458 , MSE:  tensor(261.7383, device='cuda:0') Epoch:  54\n",
            "------------------------------------------------------------\n",
            "EPOCH: 55\n",
            "TRAINING: \n",
            "LogLike/nats -11.88762474693815 Time loss  tensor(125.9207, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811953108894562 Time loss  tensor(261.5809, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 123.58305900096893\n",
            "BEST RESULTS: LLH: -14.811953108894562 , MSE:  tensor(261.5809, device='cuda:0') Epoch:  55\n",
            "------------------------------------------------------------\n",
            "EPOCH: 56\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621889255376 Time loss  tensor(125.8347, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950135583007 Time loss  tensor(261.5550, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 125.82229244709015\n",
            "BEST RESULTS: LLH: -14.811950135583007 , MSE:  tensor(261.5550, device='cuda:0') Epoch:  56\n",
            "------------------------------------------------------------\n",
            "EPOCH: 57\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621724071401 Time loss  tensor(125.7608, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811947558712992 Time loss  tensor(261.4142, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 128.03371356725694\n",
            "BEST RESULTS: LLH: -14.811947558712992 , MSE:  tensor(261.4142, device='cuda:0') Epoch:  57\n",
            "------------------------------------------------------------\n",
            "EPOCH: 58\n",
            "TRAINING: \n",
            "LogLike/nats -11.887620683412358 Time loss  tensor(125.6908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811939431661408 Time loss  tensor(261.3481, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 130.22347467740377\n",
            "BEST RESULTS: LLH: -14.811939431661408 , MSE:  tensor(261.3481, device='cuda:0') Epoch:  58\n",
            "------------------------------------------------------------\n",
            "EPOCH: 59\n",
            "TRAINING: \n",
            "LogLike/nats -11.887621624961017 Time loss  tensor(125.6158, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "VALIDATION: \n",
            "LogLike/nats -14.811950135583007 Time loss  tensor(261.2416, device='cuda:0')\n",
            "\n",
            "Training Time (in minutes) = 132.42264656623203\n",
            "BEST RESULTS: LLH: -14.811950135583007 , MSE:  tensor(261.2416, device='cuda:0') Epoch:  59\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C2FdZg8cmQB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "464fbe88-ff05-475a-9d51-5dd381d6b361"
      },
      "source": [
        "plt.figure(figsize=(20,6))\r\n",
        "\r\n",
        "plt.subplot(1,2,1)\r\n",
        "plt.plot(epoch_list,loss_list, label = 'train', c = 'b')\r\n",
        "plt.plot(epoch_list, loss_list_val, label = 'val', c = 'r')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('LogLikelihood Loss')\r\n",
        "\r\n",
        "plt.subplot(1,2,2)\r\n",
        "plt.plot(epoch_list,mse_sum_list, label = 'train', c = 'b')\r\n",
        "plt.plot(epoch_list, mse_sum_list_val, label = 'val', c = 'r')\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('MSE Loss')\r\n",
        "\r\n",
        "plt.legend()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd5258d7dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBbQq7BEStME"
      },
      "source": [
        "#torch.save(model.state_dict(), 'UNIPoint_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_F_InB3ZJ6C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
